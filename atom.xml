<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>victory的博客</title>
  
  <subtitle>长安一片月，万户捣衣声</subtitle>
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2025-02-13T13:50:00.339Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>victory-liao</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Transformer | transformer代码实现</title>
    <link href="http://example.com/2025/02/13/transformer%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/"/>
    <id>http://example.com/2025/02/13/transformer%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/</id>
    <published>2025-02-13T13:47:14.000Z</published>
    <updated>2025-02-13T13:50:00.339Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p>Transformer</p><p>Transformer是一种基于自注意力机制的深度学习模型，由Google在2017年的论文“Attention is All You Need”提出。Transformer由编码器（Encoder）和解码器（Decoder）组成，结构如下图所示：</p><p><img src="/2025/02/13/transformer%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/transformer.png"></p></li><li><p>Transformer Pytorch代码实现</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, max_len=<span class="number">5000</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len, dtype=torch.<span class="built_in">float</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).<span class="built_in">float</span>() * -(math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        self.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> x + self.pe[:, :x.size(<span class="number">1</span>)].detach()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, num_heads</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MultiHeadAttention, self).__init__()</span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        <span class="keyword">assert</span> d_model % self.num_heads == <span class="number">0</span>  <span class="comment"># 确保 d_model 能被 num_heads 整除</span></span><br><span class="line"></span><br><span class="line">        self.depth = d_model // self.num_heads</span><br><span class="line"></span><br><span class="line">        self.wq = nn.Linear(d_model, d_model)</span><br><span class="line">        self.wk = nn.Linear(d_model, d_model)</span><br><span class="line">        self.wv = nn.Linear(d_model, d_model)</span><br><span class="line"></span><br><span class="line">        self.dense = nn.Linear(d_model, d_model)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">split_heads</span>(<span class="params">self, x, batch_size</span>):</span></span><br><span class="line">        x = x.view(batch_size, -<span class="number">1</span>, self.num_heads, self.depth)</span><br><span class="line">        <span class="keyword">return</span> x.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">attention</span>(<span class="params">self, query, key, value, mask=<span class="literal">None</span>, dropout=<span class="literal">None</span></span>):</span></span><br><span class="line">        matmul_qk = torch.matmul(query, key.transpose(-<span class="number">2</span>, -<span class="number">1</span>))  <span class="comment"># QK^T</span></span><br><span class="line">        dk = query.size(-<span class="number">1</span>)</span><br><span class="line">        scaled_attention_logits = matmul_qk / math.sqrt(dk)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            scaled_attention_logits += (mask * -<span class="number">1e9</span>)  <span class="comment"># 避免pad部分被注意到</span></span><br><span class="line"></span><br><span class="line">        attention_weights = torch.softmax(scaled_attention_logits, dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            attention_weights = dropout(attention_weights)</span><br><span class="line"></span><br><span class="line">        output = torch.matmul(attention_weights, value)</span><br><span class="line">        <span class="keyword">return</span> output, attention_weights</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, query, key, value, mask=<span class="literal">None</span>, dropout=<span class="literal">None</span></span>):</span></span><br><span class="line">        batch_size = query.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        query = self.split_heads(self.wq(query), batch_size)</span><br><span class="line">        key = self.split_heads(self.wk(key), batch_size)</span><br><span class="line">        value = self.split_heads(self.wv(value), batch_size)</span><br><span class="line"></span><br><span class="line">        output, attention_weights = self.attention(query, key, value, mask, dropout)</span><br><span class="line">        output = output.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>).contiguous().view(batch_size, -<span class="number">1</span>, self.d_model)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.dense(output)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FeedForwardNetwork</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, d_ff=<span class="number">2048</span>, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(FeedForwardNetwork, self).__init__()</span><br><span class="line">        self.linear1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        self.linear2 = nn.Linear(d_ff, d_model)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = F.relu(self.linear1(x))</span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        <span class="keyword">return</span> self.linear2(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, num_heads, d_ff=<span class="number">2048</span>, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(EncoderLayer, self).__init__()</span><br><span class="line">        self.attention = MultiHeadAttention(d_model, num_heads)</span><br><span class="line">        self.ffn = FeedForwardNetwork(d_model, d_ff, dropout)</span><br><span class="line">        self.layernorm1 = nn.LayerNorm(d_model)</span><br><span class="line">        self.layernorm2 = nn.LayerNorm(d_model)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, mask=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="comment"># 自注意力层</span></span><br><span class="line">        attn_output = self.attention(x, x, x, mask, self.dropout)</span><br><span class="line">        x = self.layernorm1(x + attn_output)  <span class="comment"># 残差连接 + LayerNorm</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 前馈网络层</span></span><br><span class="line">        ffn_output = self.ffn(x)</span><br><span class="line">        x = self.layernorm2(x + ffn_output)  <span class="comment"># 残差连接 + LayerNorm</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, num_heads, d_ff=<span class="number">2048</span>, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(DecoderLayer, self).__init__()</span><br><span class="line">        self.attention1 = MultiHeadAttention(d_model, num_heads)</span><br><span class="line">        self.attention2 = MultiHeadAttention(d_model, num_heads)</span><br><span class="line">        self.ffn = FeedForwardNetwork(d_model, d_ff, dropout)</span><br><span class="line">        self.layernorm1 = nn.LayerNorm(d_model)</span><br><span class="line">        self.layernorm2 = nn.LayerNorm(d_model)</span><br><span class="line">        self.layernorm3 = nn.LayerNorm(d_model)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, enc_output, look_ahead_mask=<span class="literal">None</span>, padding_mask=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="comment"># 解码器中的自注意力层</span></span><br><span class="line">        attn1_output = self.attention1(x, x, x, look_ahead_mask, self.dropout)</span><br><span class="line">        x = self.layernorm1(attn1_output + x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 编码器-解码器注意力层</span></span><br><span class="line">        attn2_output = self.attention2(x, enc_output, enc_output, padding_mask, self.dropout)</span><br><span class="line">        x = self.layernorm2(attn2_output + x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 前馈网络层</span></span><br><span class="line">        ffn_output = self.ffn(x)</span><br><span class="line">        x = self.layernorm3(ffn_output + x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerEncoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, d_model, num_heads, num_layers, d_ff=<span class="number">2048</span>, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(TransformerEncoder, self).__init__()</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, d_model)</span><br><span class="line">        self.positional_encoding = PositionalEncoding(d_model)</span><br><span class="line">        self.layers = nn.ModuleList([</span><br><span class="line">            EncoderLayer(d_model, num_heads, d_ff, dropout) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_layers)</span><br><span class="line">        ])</span><br><span class="line">        self.d_model = d_model</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, mask=<span class="literal">None</span></span>):</span></span><br><span class="line">        x = self.embedding(x) * math.sqrt(self.d_model)  <span class="comment"># 嵌入 + 缩放</span></span><br><span class="line">        x = self.positional_encoding(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, mask)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerDecoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, d_model, num_heads, num_layers, d_ff=<span class="number">2048</span>, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(TransformerDecoder, self).__init__()</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, d_model)</span><br><span class="line">        self.positional_encoding = PositionalEncoding(d_model)</span><br><span class="line">        self.layers = nn.ModuleList([</span><br><span class="line">            DecoderLayer(d_model, num_heads, d_ff, dropout) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_layers)</span><br><span class="line">        ])</span><br><span class="line">        self.d_model = d_model</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, enc_output, look_ahead_mask=<span class="literal">None</span>, padding_mask=<span class="literal">None</span></span>):</span></span><br><span class="line">        x = self.embedding(x) * math.sqrt(self.d_model)</span><br><span class="line">        x = self.positional_encoding(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, enc_output, look_ahead_mask, padding_mask)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Transformer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, d_model, num_heads, num_layers, d_ff=<span class="number">2048</span>, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Transformer, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.encoder = TransformerEncoder(vocab_size, d_model, num_heads, num_layers, d_ff, dropout)</span><br><span class="line">        self.decoder = TransformerDecoder(vocab_size, d_model, num_heads, num_layers, d_ff, dropout)</span><br><span class="line">        self.output_layer = nn.Linear(d_model, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, src, tgt, src_mask=<span class="literal">None</span>, tgt_mask=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="comment"># 编码器部分</span></span><br><span class="line">        enc_output = self.encoder(src, src_mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 解码器部分</span></span><br><span class="line">        dec_output = self.decoder(tgt, enc_output, tgt_mask, src_mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 输出层</span></span><br><span class="line">        <span class="keyword">return</span> self.output_layer(dec_output)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">vocab_size = <span class="number">10000</span>  <span class="comment"># 词汇表大小</span></span><br><span class="line">d_model = <span class="number">512</span>  <span class="comment"># 特征维度</span></span><br><span class="line">num_heads = <span class="number">8</span>  <span class="comment"># 注意力头数</span></span><br><span class="line">num_layers = <span class="number">6</span>  <span class="comment"># 编码器和解码器层数</span></span><br><span class="line">dropout = <span class="number">0.1</span>  <span class="comment"># Dropout 比例</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化 Transformer 模型</span></span><br><span class="line">transformer = Transformer(vocab_size, d_model, num_heads, num_layers, dropout=dropout)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入张量（batch_size, sequence_length）</span></span><br><span class="line">src = torch.randint(<span class="number">0</span>, vocab_size, (<span class="number">32</span>, <span class="number">100</span>))  <span class="comment"># 假设 source 语言输入 batch_size=32，序列长度=100</span></span><br><span class="line">tgt = torch.randint(<span class="number">0</span>, vocab_size, (<span class="number">32</span>, <span class="number">100</span>))  <span class="comment"># 假设 target 语言输入 batch_size=32，序列长度=100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建遮罩（假设没有 padding）</span></span><br><span class="line">src_mask = <span class="literal">None</span></span><br><span class="line">tgt_mask = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line">output = transformer(src, tgt, src_mask, tgt_mask)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Output shape:&quot;</span>, output.shape)  <span class="comment"># 输出的形状 (batch_size, tgt_sequence_length, vocab_size)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Transformer&lt;/p&gt;
&lt;p&gt;Transformer是一种基于自注意力机制的深度学习模型，由Google在2017年的论文“Attention is All You Need”提出。Transformer由编码器（Encoder）和解码器（Deco</summary>
      
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    <category term="深度学习" scheme="http://example.com/categories/AI/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Transformer" scheme="http://example.com/categories/AI/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Transformer/"/>
    
    <category term="transformer代码实现" scheme="http://example.com/categories/AI/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Transformer/transformer%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/"/>
    
    
    <category term="Transformer" scheme="http://example.com/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>Transformer | 注意力机制代码实现</title>
    <link href="http://example.com/2025/02/13/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/"/>
    <id>http://example.com/2025/02/13/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/</id>
    <published>2025-02-13T13:31:36.000Z</published>
    <updated>2025-02-13T13:46:50.698Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p>自注意力机制（Self-Attention）</p><p>自注意力机制是Transfromer中的重要组件，它通过计算Query (Q)、Key (K)、Value (V)获取token之间的相关性。Q、K、V矩阵是通过输入嵌入（或前一层的输出）与权重矩阵进行线性变换得到的。</p><p>自注意力机制的输入格式为（batch_size, seq_len, d_model）,batch_size是批次大小，seq_len是序列长度，d_model是嵌入维度。</p><p>Q、K、V的计算：Q=X x W_Q, K=X x W_K, V=X x W_V。</p><p>自注意力输出的计算：output = (QxK_T) x V。</p></li><li><p>自注意力机制pytorch代码实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SingleHeadAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, embed_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(SingleHeadAttention, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 输入的embedding维度</span></span><br><span class="line">        self.embed_size = embed_size</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义查询、键和值的线性变换</span></span><br><span class="line">        self.query_fc = nn.Linear(embed_size, embed_size)</span><br><span class="line">        self.key_fc = nn.Linear(embed_size, embed_size)</span><br><span class="line">        self.value_fc = nn.Linear(embed_size, embed_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 输出的线性变换</span></span><br><span class="line">        self.out_fc = nn.Linear(embed_size, embed_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, mask=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;X.shape: &quot;</span>, X.shape)</span><br><span class="line">        <span class="comment"># Step1: 通过线性层生成查询、键和值的向量</span></span><br><span class="line">        Q = self.query_fc(X)  <span class="comment"># (batch_size, seq_len, embed_size)</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Q.shape: &quot;</span>, Q.shape)</span><br><span class="line">        K = self.key_fc(X)  <span class="comment"># (batch_size, seq_len, embed_size)</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;K.shape: &quot;</span>, K.shape)</span><br><span class="line">        V = self.value_fc(X)  <span class="comment"># (batch_size, seq_len, embed_size)</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;V.shape: &quot;</span>, V.shape)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step2: 计算注意力得分</span></span><br><span class="line">        attention_scores = torch.matmul(Q, K.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / (self.embed_size ** <span class="number">0.5</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;attention_scores.shape: &quot;</span>, attention_scores.shape)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果有mask，应用mask</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            attention_scores = attention_scores.masked_fill(mask == <span class="number">0</span>, <span class="built_in">float</span>(<span class="string">&#x27;-inf&#x27;</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step3: 计算注意力权重(softmax)</span></span><br><span class="line">        attention_weights = F.softmax(attention_scores, dim=-<span class="number">1</span>)  <span class="comment"># (batch_size, seq_len, seq_len)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step4: 加权求和得到输出</span></span><br><span class="line">        output = torch.matmul(attention_weights, V)  <span class="comment"># (batch_size, seq_len, embed_size)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    batch_size = <span class="number">2</span></span><br><span class="line">    seq_len = <span class="number">4</span></span><br><span class="line">    embed_size = <span class="number">8</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 随机生成输入数据</span></span><br><span class="line">    X = torch.randn(batch_size, seq_len, embed_size)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建自注意力模型</span></span><br><span class="line">    attention_layer = SingleHeadAttention(embed_size)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 前向传播</span></span><br><span class="line">    output = attention_layer(X)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Output: <span class="subst">&#123;output.shape&#125;</span>&quot;</span>)  <span class="comment"># (batch_size, seq_len, embed_size)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li><p>多头注意力机制（Multi-Head Attention）</p><p>多头注意力机制是Transformer模型中的一个核心组成部分，它通过并行计算多个注意力头来捕捉输入序列的不同信息，每个注意力头都有独立的Q、K、V，能够关注输入的不同子空间，从而增强模型对不同特征的表达能力。</p><p>多头注意力计算过程：</p><ol><li>线性变换：输入的向量首先会通过不同的线性变换（权重矩阵）生成多个查询（Q）、键（K）和值（V）向量。</li><li>计算注意力：每个注意力头根据查询、键和值计算注意力权重，并通过加权求和得到一个输出。</li><li>拼接：所有头的输出会被拼接在一起。</li><li>线性变换：拼接后的结果通过一个线性变换，最终输出。</li></ol></li><li><p>多头注意力机制pytorch代码实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, embed_size, num_heads</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MultiHeadAttention, self).__init__()</span><br><span class="line">        self.embed_size = embed_size</span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        self.head_dim = embed_size // num_heads</span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> self.head_dim * num_heads == embed_size, <span class="string">&quot;Embedding size must be divisible by num_heads&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义查询、键、值的线性变换</span></span><br><span class="line">        self.query_fc = nn.Linear(embed_size, embed_size)</span><br><span class="line">        self.key_fc = nn.Linear(embed_size, embed_size)</span><br><span class="line">        self.value_fc = nn.Linear(embed_size, embed_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义输出的线性变换</span></span><br><span class="line">        self.fc_out = nn.Linear(embed_size, embed_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        batch_size = X.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 通过线性变换得到 Q, K, V</span></span><br><span class="line">        Q = self.query_fc(X)  <span class="comment"># (seq_len, batch_size, embed_size)</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Q.shape: &quot;</span>, Q.shape)</span><br><span class="line">        K = self.key_fc(X)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;K.shape: &quot;</span>, K.shape)</span><br><span class="line">        V = self.value_fc(X)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;V.shape: &quot;</span>, V.shape)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将Q, K, V 切分成多个头</span></span><br><span class="line">        Q = Q.view(X.shape[<span class="number">0</span>], batch_size, self.num_heads, self.head_dim).transpose(<span class="number">1</span>,</span><br><span class="line">                                                                                    <span class="number">2</span>)  <span class="comment"># (seq_len, batch_size, num_heads, head_dim)</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Q_multi_head.shape: &quot;</span>, Q.shape)</span><br><span class="line">        K = K.view(X.shape[<span class="number">0</span>], batch_size, self.num_heads, self.head_dim).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;K_multi_head.shape: &quot;</span>, K.shape)</span><br><span class="line">        V = V.view(X.shape[<span class="number">0</span>], batch_size, self.num_heads, self.head_dim).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;V_multi_head.shape: &quot;</span>, V.shape)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算注意力得分</span></span><br><span class="line">        energy = torch.matmul(Q, K.transpose(-<span class="number">2</span>, -<span class="number">1</span>))  <span class="comment"># (seq_len, batch_size, num_heads, seq_len)</span></span><br><span class="line">        attention = torch.softmax(energy / (self.head_dim ** <span class="number">0.5</span>), dim=-<span class="number">1</span>)  <span class="comment"># 注意力得分</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Q*K.shape&quot;</span>, attention.shape)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算加权求和的输出</span></span><br><span class="line">        out = torch.matmul(attention, V)  <span class="comment"># (seq_len, batch_size, num_heads, head_dim)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将多个头合并</span></span><br><span class="line">        out = out.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(X.shape[<span class="number">0</span>], batch_size, self.num_heads * self.head_dim)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 通过输出的线性层</span></span><br><span class="line">        out = self.fc_out(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line">embed_size = <span class="number">64</span></span><br><span class="line">num_heads = <span class="number">8</span></span><br><span class="line">seq_len = <span class="number">10</span></span><br><span class="line">batch_size = <span class="number">32</span></span><br><span class="line"></span><br><span class="line">multihead_attention = MultiHeadAttention(embed_size, num_heads)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入张量，shape: (seq_len, batch_size, embed_size)</span></span><br><span class="line">X = torch.rand(seq_len, batch_size, embed_size)</span><br><span class="line"></span><br><span class="line">out = multihead_attention(X)</span><br><span class="line"><span class="built_in">print</span>(out.shape)  <span class="comment"># (seq_len, batch_size, embed_size)</span></span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;p&gt;自注意力机制（Self-Attention）&lt;/p&gt;
&lt;p&gt;自注意力机制是Transfromer中的重要组件，它通过计算Query (Q)、Key (K)、Value (V)获取token之间的相关性。Q、K、V矩阵是通过输入嵌入（或前一层的输出）与权重矩</summary>
      
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    <category term="深度学习" scheme="http://example.com/categories/AI/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Transformer" scheme="http://example.com/categories/AI/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Transformer/"/>
    
    <category term="注意力机制代码实现" scheme="http://example.com/categories/AI/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Transformer/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/"/>
    
    
    <category term="Transformer" scheme="http://example.com/tags/Transformer/"/>
    
    <category term="注意力机制" scheme="http://example.com/tags/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"/>
    
  </entry>
  
  <entry>
    <title>Tools | AI模型可视化工具Netron</title>
    <link href="http://example.com/2025/02/10/AI%E6%A8%A1%E5%9E%8B%E5%8F%AF%E8%A7%86%E5%8C%96%E5%B7%A5%E5%85%B7Netron/"/>
    <id>http://example.com/2025/02/10/AI%E6%A8%A1%E5%9E%8B%E5%8F%AF%E8%A7%86%E5%8C%96%E5%B7%A5%E5%85%B7Netron/</id>
    <published>2025-02-10T12:08:51.000Z</published>
    <updated>2025-02-10T12:12:12.030Z</updated>
    
    <content type="html"><![CDATA[<p>Netron是一款开源的深度学习模型可视化工具，支持多种深度学习框架生成的模型（例如，PyTorch、TensorFlow、ONNX等）的可视化。</p><p>网页版工具地址：<a href="https://netron.app/">Netron</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Netron是一款开源的深度学习模型可视化工具，支持多种深度学习框架生成的模型（例如，PyTorch、TensorFlow、ONNX等）的可视化。&lt;/p&gt;
&lt;p&gt;网页版工具地址：&lt;a href=&quot;https://netron.app/&quot;&gt;Netron&lt;/a&gt;&lt;/p&gt;
</summary>
      
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    <category term="Tools" scheme="http://example.com/categories/AI/Tools/"/>
    
    <category term="Netron" scheme="http://example.com/categories/AI/Tools/Netron/"/>
    
    
    <category term="AI" scheme="http://example.com/tags/AI/"/>
    
    <category term="可视化" scheme="http://example.com/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
    <category term="Netron" scheme="http://example.com/tags/Netron/"/>
    
  </entry>
  
  <entry>
    <title>GPU | FMA</title>
    <link href="http://example.com/2024/09/13/FMA/"/>
    <id>http://example.com/2024/09/13/FMA/</id>
    <published>2024-09-13T13:23:50.000Z</published>
    <updated>2024-09-13T13:39:30.622Z</updated>
    
    <content type="html"><![CDATA[<p>什么是FMA？</p><span id="more"></span><p>GPU能够加速AI模型训练和推理速度的原因是GPU拥有众多的CUDA core、Tensor Core。</p><p>AI模型的核心计算是矩阵乘加运算，Tensor Core实现了不同浮点精度的矩阵乘加运算（FMA），从而加速了AI模型的训练、推理过程。</p><p><img src="/2024/09/13/FMA/1.png"></p><p>下面是使用FMA进行矩阵乘加运算D = A * B + C的CUDA示例代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> </span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 假设矩阵A, B, C和D都是NxN大小的方阵</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> N 1024</span></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">matrixFMA</span><span class="params">(<span class="keyword">float</span> *A, <span class="keyword">float</span> *B, <span class="keyword">float</span> *C, <span class="keyword">float</span> *D, <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> row = blockIdx.y * blockDim.y + threadIdx.y;</span><br><span class="line">    <span class="keyword">int</span> col = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (row &lt; n &amp;&amp; col &lt; n) &#123;</span><br><span class="line">        <span class="keyword">float</span> sum = <span class="number">0.0f</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> k = <span class="number">0</span>; k &lt; n; ++k) &#123;</span><br><span class="line">            <span class="comment">// 使用FMA操作计算每个元素</span></span><br><span class="line">            sum = __fmaf_rn(A[row * n + k], B[k * n + col], sum);</span><br><span class="line">        &#125;</span><br><span class="line">        D[row * n + col] = sum + C[row * n + col];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">float</span> *A, *B, *C, *D;</span><br><span class="line">    <span class="keyword">float</span> *d_A, *d_B, *d_C, *d_D;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 分配主机内存</span></span><br><span class="line">    A = <span class="keyword">new</span> <span class="keyword">float</span>[N * N];</span><br><span class="line">    B = <span class="keyword">new</span> <span class="keyword">float</span>[N * N];</span><br><span class="line">    C = <span class="keyword">new</span> <span class="keyword">float</span>[N * N];</span><br><span class="line">    D = <span class="keyword">new</span> <span class="keyword">float</span>[N * N];</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 初始化输入数据</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; N * N; i++) &#123;</span><br><span class="line">        A[i] = <span class="built_in"><span class="keyword">static_cast</span></span>(i);</span><br><span class="line">        B[i] = <span class="built_in"><span class="keyword">static_cast</span></span>(<span class="number">2</span> * i);</span><br><span class="line">        C[i] = <span class="built_in"><span class="keyword">static_cast</span></span>(<span class="number">3</span> * i);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 分配设备内存</span></span><br><span class="line">    <span class="built_in">cudaMalloc</span>(&amp;d_A, N * N * <span class="built_in"><span class="keyword">sizeof</span></span>(<span class="keyword">float</span>));</span><br><span class="line">    <span class="built_in">cudaMalloc</span>(&amp;d_B, N * N * <span class="built_in"><span class="keyword">sizeof</span></span>(<span class="keyword">float</span>));</span><br><span class="line">    <span class="built_in">cudaMalloc</span>(&amp;d_C, N * N * <span class="built_in"><span class="keyword">sizeof</span></span>(<span class="keyword">float</span>));</span><br><span class="line">    <span class="built_in">cudaMalloc</span>(&amp;d_D, N * N * <span class="built_in"><span class="keyword">sizeof</span></span>(<span class="keyword">float</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将输入数据复制到设备</span></span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(d_A, A, N * N * <span class="built_in"><span class="keyword">sizeof</span></span>(<span class="keyword">float</span>), cudaMemcpyHostToDevice);</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(d_B, B, N * N * <span class="built_in"><span class="keyword">sizeof</span></span>(<span class="keyword">float</span>), cudaMemcpyHostToDevice);</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(d_C, C, N * N * <span class="built_in"><span class="keyword">sizeof</span></span>(<span class="keyword">float</span>), cudaMemcpyHostToDevice);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 配置并启动内核</span></span><br><span class="line">    <span class="keyword">int</span> threadsPerBlock = <span class="number">16</span>;</span><br><span class="line">    <span class="keyword">int</span> blocksPerGrid = (N + threadsPerBlock - <span class="number">1</span>) / threadsPerBlock;</span><br><span class="line">    <span class="function">dim3 <span class="title">dimGrid</span><span class="params">(blocksPerGrid, blocksPerGrid, <span class="number">1</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">dimBlock</span><span class="params">(threadsPerBlock, threadsPerBlock, <span class="number">1</span>)</span></span>;</span><br><span class="line">    matrixFMA&lt;&lt;&gt;&gt;(d_A, d_B, d_C, d_D, N);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将结果从设备复制回主机</span></span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(D, d_D, N * N * <span class="built_in"><span class="keyword">sizeof</span></span>(<span class="keyword">float</span>), cudaMemcpyDeviceToHost);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 验证结果（可选）</span></span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 释放内存</span></span><br><span class="line">    <span class="built_in">cudaFree</span>(d_A);</span><br><span class="line">    <span class="built_in">cudaFree</span>(d_B);</span><br><span class="line">    <span class="built_in">cudaFree</span>(d_C);</span><br><span class="line">    <span class="built_in">cudaFree</span>(d_D);</span><br><span class="line">    <span class="keyword">delete</span>[] A;</span><br><span class="line">    <span class="keyword">delete</span>[] B;</span><br><span class="line">    <span class="keyword">delete</span>[] C;</span><br><span class="line">    <span class="keyword">delete</span>[] D;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="/2024/09/13/FMA/2.png"></p><ul><li><p>FMA与Tensor Core</p><p>Tensor Core 采用融合乘法加法（FMA）的方式来高效地处理计算任务。每个 Tensor Core 每周期能执行 <strong>4x4x4 GEMM</strong>，64 个浮点乘法累加（FMA）运算。</p></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;什么是FMA？&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    <category term="AI基础设施" scheme="http://example.com/categories/AI/AI%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/"/>
    
    <category term="GPU" scheme="http://example.com/categories/AI/AI%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/GPU/"/>
    
    <category term="FMA" scheme="http://example.com/categories/AI/AI%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/GPU/FMA/"/>
    
    
    <category term="Tensor Core" scheme="http://example.com/tags/Tensor-Core/"/>
    
    <category term="FMA" scheme="http://example.com/tags/FMA/"/>
    
  </entry>
  
  <entry>
    <title>AI | AI服务器高速互联技术</title>
    <link href="http://example.com/2024/08/29/AI%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%AB%98%E9%80%9F%E4%BA%92%E8%81%94%E6%8A%80%E6%9C%AF/"/>
    <id>http://example.com/2024/08/29/AI%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%AB%98%E9%80%9F%E4%BA%92%E8%81%94%E6%8A%80%E6%9C%AF/</id>
    <published>2024-08-29T12:41:23.000Z</published>
    <updated>2024-08-29T13:10:33.646Z</updated>
    
    <content type="html"><![CDATA[<p>三种 RDMA技术RoCE、iWARP，后两者是基于以太网的技术，IB的链路层进行了重新设计。</p><p>随着大模型以及AIGC的快速发展，AI对于算力有了更高的要求。从最开始使用单机单卡（CPU+GPU）进行DL模型训练推理，到使用单机多卡（CPU+GPUs），再到多机多卡的AI集群。</p><span id="more"></span><p>集中更多的算力可以加速AI模型的训练，但只有强大的AI算力是不够的，AI模型的训练是从大量数据中学习规律、学习知识，AI的训练过程设计大量的数据搬运，需要高带宽、低延时的数据传输。高速互联技术能够将不同的算力芯片或服务器连接在一起组成一个算力网络，并提供高速的数据传输能力。</p><p>高速互联分为结点内部计算设备的互联与结点间的互联。结点内互联又分为单节点互联与超级结点互联。单节点互联能够实现两个计算设备的互联，例如CPU与CPU之间通过UPI（Ultra Path Interconnect）进行连接，GPU与GPU可以通过PCIe（Intel于2001年开发）/<a href="https://www.nvidia.cn/design-visualization/nvlink-bridges/">NVLink（Nvidia开发）</a>/Infinity Fabric（AMD开发）进行互联，CPU与GPU之间通过PCIe或者<a href="https://zhuanlan.zhihu.com/p/676847465">CXL（Compute Express Link,Intel于2019年提出的高速互联协议）</a>进行互联。超级结点互联能够实现多个计算设备的互联，例如多GPU间的互联可以使用NVLink Switch或RDMA进行互联。</p><p>机间互联通过RDMA（Remote Direct Memory Access）进行互联，RDMA常见的有三种实现：InfiniBand、RoCE、iWarp，使用最为广泛的是IB和RoCE（RoCEv2），RoCE、iWARP，后两者是基于以太网的技术，IB的链路层进行了重新设计。<img src="/2024/08/29/AI%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%AB%98%E9%80%9F%E4%BA%92%E8%81%94%E6%8A%80%E6%9C%AF/%E9%AB%98%E9%80%9F%E4%BA%92%E8%81%94%E6%8A%80%E6%9C%AF.png"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;三种 RDMA技术RoCE、iWARP，后两者是基于以太网的技术，IB的链路层进行了重新设计。&lt;/p&gt;
&lt;p&gt;随着大模型以及AIGC的快速发展，AI对于算力有了更高的要求。从最开始使用单机单卡（CPU+GPU）进行DL模型训练推理，到使用单机多卡（CPU+GPUs），再到多机多卡的AI集群。&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    <category term="高速互联技术" scheme="http://example.com/categories/AI/%E9%AB%98%E9%80%9F%E4%BA%92%E8%81%94%E6%8A%80%E6%9C%AF/"/>
    
    <category term="AI服务器高速互联技术分类" scheme="http://example.com/categories/AI/%E9%AB%98%E9%80%9F%E4%BA%92%E8%81%94%E6%8A%80%E6%9C%AF/AI%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%AB%98%E9%80%9F%E4%BA%92%E8%81%94%E6%8A%80%E6%9C%AF%E5%88%86%E7%B1%BB/"/>
    
    
    <category term="AI" scheme="http://example.com/tags/AI/"/>
    
    <category term="高速互联技术" scheme="http://example.com/tags/%E9%AB%98%E9%80%9F%E4%BA%92%E8%81%94%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title>指令集 | CISC与RISC指令集的比较</title>
    <link href="http://example.com/2024/08/29/CISC%E4%B8%8ERISC%E6%8C%87%E4%BB%A4%E9%9B%86%E7%9A%84%E6%AF%94%E8%BE%83/"/>
    <id>http://example.com/2024/08/29/CISC%E4%B8%8ERISC%E6%8C%87%E4%BB%A4%E9%9B%86%E7%9A%84%E6%AF%94%E8%BE%83/</id>
    <published>2024-08-29T12:33:38.000Z</published>
    <updated>2024-08-29T12:40:06.272Z</updated>
    
    <content type="html"><![CDATA[<p>指令集定义了CPU可以执行的指令集合。指令集从复杂度分类可分为CISC和RISC指令集。CISC指令集最常见的是X86，Intel与AMD两大CPU巨头生产的CPU以X86架构为主。RISC指令集有Arm、RISC-V、MIPS、Alpha等，Arm指令集主要应用于移动端、嵌入式计算芯片。</p><span id="more"></span><p>以下是两种不同指令集的比较：</p><table><thead><tr><th align="center"></th><th align="center">CISC</th><th align="center">RISC</th></tr></thead><tbody><tr><td align="center">指令系统</td><td align="center">复杂，庞大</td><td align="center">简单，精简</td></tr><tr><td align="center">指令数量</td><td align="center"><code>&gt;200</code></td><td align="center"><code>&lt;100</code></td></tr><tr><td align="center">指令长度</td><td align="center">不定长</td><td align="center">定长</td></tr><tr><td align="center">可访存指令</td><td align="center">不加限制</td><td align="center">只有load/store指令</td></tr><tr><td align="center">指令执行时间</td><td align="center">相差较大</td><td align="center">大部分在一个周期内完成</td></tr><tr><td align="center">指令使用频率</td><td align="center">相差较大</td><td align="center">都比较常用</td></tr><tr><td align="center">通用寄存器数</td><td align="center">较少</td><td align="center">多</td></tr><tr><td align="center">目标代码</td><td align="center">难以利用编译优化生成高效的目标代码程序</td><td align="center">可采用编译优化生成高效执行的代码</td></tr><tr><td align="center">控制方式</td><td align="center">微程序控制</td><td align="center">组合逻辑控制</td></tr><tr><td align="center">指令流水</td><td align="center">可通过一定方式实现</td><td align="center">必须实现</td></tr></tbody></table>]]></content>
    
    
    <summary type="html">&lt;p&gt;指令集定义了CPU可以执行的指令集合。指令集从复杂度分类可分为CISC和RISC指令集。CISC指令集最常见的是X86，Intel与AMD两大CPU巨头生产的CPU以X86架构为主。RISC指令集有Arm、RISC-V、MIPS、Alpha等，Arm指令集主要应用于移动端、嵌入式计算芯片。&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    <category term="AI基础设施" scheme="http://example.com/categories/AI/AI%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/"/>
    
    <category term="CPU" scheme="http://example.com/categories/AI/AI%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/CPU/"/>
    
    <category term="指令集" scheme="http://example.com/categories/AI/AI%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/CPU/%E6%8C%87%E4%BB%A4%E9%9B%86/"/>
    
    <category term="CISC与RISC指令集的比较" scheme="http://example.com/categories/AI/AI%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/CPU/%E6%8C%87%E4%BB%A4%E9%9B%86/CISC%E4%B8%8ERISC%E6%8C%87%E4%BB%A4%E9%9B%86%E7%9A%84%E6%AF%94%E8%BE%83/"/>
    
    
    <category term="指令集" scheme="http://example.com/tags/%E6%8C%87%E4%BB%A4%E9%9B%86/"/>
    
    <category term="ISA" scheme="http://example.com/tags/ISA/"/>
    
    <category term="CISC" scheme="http://example.com/tags/CISC/"/>
    
    <category term="RISC" scheme="http://example.com/tags/RISC/"/>
    
  </entry>
  
  <entry>
    <title>AI模型轻量化 | 模型蒸馏</title>
    <link href="http://example.com/2024/08/29/%E6%A8%A1%E5%9E%8B%E8%92%B8%E9%A6%8F/"/>
    <id>http://example.com/2024/08/29/%E6%A8%A1%E5%9E%8B%E8%92%B8%E9%A6%8F/</id>
    <published>2024-08-29T12:29:59.000Z</published>
    <updated>2024-08-29T12:33:05.993Z</updated>
    
    <content type="html"><![CDATA[<p><code>模型蒸馏</code>的核心思想是在保持较高预测性能的同时，通过知识迁移的方式，将一个复杂的大模型（<code>教师模型</code>）的知识传授给一个相对简单的小模型（<code>学生模型</code>），极大地降低了模型的复杂性和计算资源需求，实现了模型的轻量化和高效化。</p><span id="more"></span><p>以下是一个简单的模型蒸馏代码示例，使用PyTorch框架实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义教师模型和学生模型</span></span><br><span class="line">teacher_model = model_xxx</span><br><span class="line">student_model = model_xxx</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数和优化器</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer_teacher = optim.optimizer1</span><br><span class="line">optimizer_student = optim.optimizer2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练数据集</span></span><br><span class="line">trainset = datasets.data_xxx</span><br><span class="line">trainloader = torch.utils.data.DataLoader()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 蒸馏过程</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(NEPOCH):</span><br><span class="line">    running_loss_teacher = <span class="number">0.0</span></span><br><span class="line">    running_loss_student = <span class="number">0.0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> trainloader:</span><br><span class="line">        <span class="comment"># 教师模型的前向传播</span></span><br><span class="line">        outputs_teacher = teacher_model(inputs)</span><br><span class="line">        loss_teacher = criterion(outputs_teacher, labels)</span><br><span class="line">        running_loss_teacher += loss_teacher.item()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 学生模型的前向传播</span></span><br><span class="line">        outputs_student = student_model(inputs)</span><br><span class="line">        loss_student = criterion(outputs_student, labels) + <span class="number">0.1</span> * torch.<span class="built_in">sum</span>((outputs_teacher - outputs_student) ** <span class="number">2</span>)</span><br><span class="line">        running_loss_student += loss_student.item()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 反向传播和参数更新</span></span><br><span class="line">        optimizer_teacher.zero_grad()</span><br><span class="line">        optimizer_student.zero_grad()</span><br><span class="line">        loss_teacher.backward()</span><br><span class="line">        optimizer_teacher.step()</span><br><span class="line">        loss_student.backward()</span><br><span class="line">        optimizer_student.step()</span><br></pre></td></tr></table></figure><p>参考链接1：<a href="https://blog.csdn.net/qq_42533357/article/details/137026170">深度学习中的模型蒸馏技术：实现流程、作用及实践案例-CSDN博客</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;code&gt;模型蒸馏&lt;/code&gt;的核心思想是在保持较高预测性能的同时，通过知识迁移的方式，将一个复杂的大模型（&lt;code&gt;教师模型&lt;/code&gt;）的知识传授给一个相对简单的小模型（&lt;code&gt;学生模型&lt;/code&gt;），极大地降低了模型的复杂性和计算资源需求，实现了模型的轻量化和高效化。&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    <category term="AI模型轻量化" scheme="http://example.com/categories/AI/AI%E6%A8%A1%E5%9E%8B%E8%BD%BB%E9%87%8F%E5%8C%96/"/>
    
    <category term="模型蒸馏" scheme="http://example.com/categories/AI/AI%E6%A8%A1%E5%9E%8B%E8%BD%BB%E9%87%8F%E5%8C%96/%E6%A8%A1%E5%9E%8B%E8%92%B8%E9%A6%8F/"/>
    
    
    <category term="模型蒸馏" scheme="http://example.com/tags/%E6%A8%A1%E5%9E%8B%E8%92%B8%E9%A6%8F/"/>
    
    <category term="轻量化" scheme="http://example.com/tags/%E8%BD%BB%E9%87%8F%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>深度学习编译器 | 深度学习编译器与推理引擎的区别</title>
    <link href="http://example.com/2024/08/23/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E5%99%A8%E4%B8%8E%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
    <id>http://example.com/2024/08/23/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E5%99%A8%E4%B8%8E%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E%E7%9A%84%E5%8C%BA%E5%88%AB/</id>
    <published>2024-08-23T14:38:54.000Z</published>
    <updated>2024-08-29T12:17:17.479Z</updated>
    
    <content type="html"><![CDATA[<p>AI编译器与推理引擎的区别。</p><span id="more"></span><p><img src="/2024/08/23/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E5%99%A8%E4%B8%8E%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E%E7%9A%84%E5%8C%BA%E5%88%AB/1.png"></p><p><img src="/2024/08/23/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E5%99%A8%E4%B8%8E%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E%E7%9A%84%E5%8C%BA%E5%88%AB/2.png"></p><p><img src="/2024/08/23/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E5%99%A8%E4%B8%8E%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E%E7%9A%84%E5%8C%BA%E5%88%AB/3.png"></p><p>从整体架构图可以看到，<strong>AI编译器就是针对具体的AI加速芯片硬件，对上层用户接触到的高级语言进行编译，为AI流程实现更加高效的执行，高级语言在AI流程表示的优化是AI编译器的重点。</strong></p><p><img src="/2024/08/23/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E5%99%A8%E4%B8%8E%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E%E7%9A%84%E5%8C%BA%E5%88%AB/4.png"></p><p>从整体架构图可以看到，<strong>图优化在推理引擎中占了很小的一部分，推理引擎聚焦于Runtime执行部分和Kernel算子内核层，为不同的硬件提供更加高效、快捷的执行Engine。</strong></p><p>参考链接1：<a href="https://zhuanlan.zhihu.com/p/669347560">AI编译器技术剖析（一）-概述 - 知乎 (zhihu.com)</a></p><p>参考链接2：<a href="https://zhuanlan.zhihu.com/p/629048218">AI编译器和推理引擎的区别 - 知乎 (zhihu.com)</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;AI编译器与推理引擎的区别。&lt;/p&gt;</summary>
    
    
    
    <category term="编译器" scheme="http://example.com/categories/%E7%BC%96%E8%AF%91%E5%99%A8/"/>
    
    <category term="深度学习编译器" scheme="http://example.com/categories/%E7%BC%96%E8%AF%91%E5%99%A8/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E5%99%A8/"/>
    
    <category term="深度学习编译器与推理引擎的区别" scheme="http://example.com/categories/%E7%BC%96%E8%AF%91%E5%99%A8/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E5%99%A8/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E5%99%A8%E4%B8%8E%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
    
    
    <category term="AI" scheme="http://example.com/tags/AI/"/>
    
    <category term="编译器" scheme="http://example.com/tags/%E7%BC%96%E8%AF%91%E5%99%A8/"/>
    
    <category term="推理引擎" scheme="http://example.com/tags/%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/"/>
    
  </entry>
  
  <entry>
    <title>VPN | SSL VPN</title>
    <link href="http://example.com/2024/08/23/SSLVPN/"/>
    <id>http://example.com/2024/08/23/SSLVPN/</id>
    <published>2024-08-23T14:33:17.000Z</published>
    <updated>2024-08-29T12:18:48.198Z</updated>
    
    <content type="html"><![CDATA[<p><code>SSL VPN</code>是以<code>SSL</code>加密技术为基础的<code>VPN</code>技术，利用<code>SSL</code>提供的安全机制，为用户<code>远程访问</code>公司内部网络提供了安全保证。</p><p>SSL VPN的典型组网架构如下：</p><p><img src="/2024/08/23/SSLVPN/1.png"></p><span id="more"></span><p>SSL VPN的三种接入方式：</p><ol><li><p>Web接入方式</p><p><img src="/2024/08/23/SSLVPN/2.png"></p></li><li><p>TCP接入方式</p><p><img src="/2024/08/23/SSLVPN/3.png"></p></li><li><p>IP接入方式</p><p><img src="/2024/08/23/SSLVPN/4.png"></p></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;code&gt;SSL VPN&lt;/code&gt;是以&lt;code&gt;SSL&lt;/code&gt;加密技术为基础的&lt;code&gt;VPN&lt;/code&gt;技术，利用&lt;code&gt;SSL&lt;/code&gt;提供的安全机制，为用户&lt;code&gt;远程访问&lt;/code&gt;公司内部网络提供了安全保证。&lt;/p&gt;
&lt;p&gt;SSL VPN的典型组网架构如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2024/08/23/SSLVPN/1.png&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="计算机基础" scheme="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"/>
    
    <category term="计算机网络" scheme="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    <category term="VPN" scheme="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/VPN/"/>
    
    <category term="SSL VPN" scheme="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/VPN/SSL-VPN/"/>
    
    
    <category term="VPN" scheme="http://example.com/tags/VPN/"/>
    
    <category term="SSL VPN" scheme="http://example.com/tags/SSL-VPN/"/>
    
  </entry>
  
  <entry>
    <title>智算网络 | Scale up和Scale out网络</title>
    <link href="http://example.com/2024/08/23/Scaleup%E5%92%8CScaleout%E7%BD%91%E7%BB%9C/"/>
    <id>http://example.com/2024/08/23/Scaleup%E5%92%8CScaleout%E7%BD%91%E7%BB%9C/</id>
    <published>2024-08-23T14:29:48.000Z</published>
    <updated>2024-08-23T14:31:41.943Z</updated>
    
    <content type="html"><![CDATA[<p>智算网络包含Scale-up网络和Scale-out网络两张网络。</p><span id="more"></span><p>Scale-up网络描述的是单个机器内GPU、CPU、内存等连接在一起构成的网络，着重单机性能的提升，例如通过增加GPU的数量或增加CPU的数量、增加内存容量来提升单机的计算效率与吞吐量。单机内部不同芯片的连接采用PCIe，GPU之间的连接也可通过NVLink进行连接。</p><p>Scale-out网络描述的是多个算力机器连接起来构成的网络，属于机间互联，目的是为了突破单机性能，通过机间互联合并算力组成一个大的算力网络为大数据处理、大模型训练提供支持。不同机器间的连接可采用RDMA（比较高效的两种是RoCEv2和IB）。</p><p>参考链接1：<a href="https://mp.weixin.qq.com/s/X9if693QD1w3rU3RNDy2Nw">智算网络中Scale-out网络和Scale-up网络的本质区别是什么？</a></p><p>参考链接2：<a href="https://mp.weixin.qq.com/s/fyPFr6aBds3dIV2sc1B6Nw">用于智算场景的Scale-up互联技术分析</a></p><p>参考链接3：<a href="https://mp.weixin.qq.com/s/b6Qf8MD-FG1Ve_PlUyFb2g">CXL，AI时代的“运力”引擎</a></p><p>参考链接4：<a href="https://mp.weixin.qq.com/s/kJZFzX0rWiPtxMkrI8i6TA">Scale-up与Scale-out有什么不同？</a></p><p>参考链接5：<a href="https://mp.weixin.qq.com/s/RyApSIT-wyrEzbiWEsvgZQ">AIGC为什么要区分Scale-out和Scale-up两张网络？</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;智算网络包含Scale-up网络和Scale-out网络两张网络。&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    <category term="AI基础设施" scheme="http://example.com/categories/AI/AI%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/"/>
    
    <category term="智算中心" scheme="http://example.com/categories/AI/AI%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/%E6%99%BA%E7%AE%97%E4%B8%AD%E5%BF%83/"/>
    
    <category term="智算网络：Scale up和Scale out网络" scheme="http://example.com/categories/AI/AI%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/%E6%99%BA%E7%AE%97%E4%B8%AD%E5%BF%83/%E6%99%BA%E7%AE%97%E7%BD%91%E7%BB%9C%EF%BC%9AScale-up%E5%92%8CScale-out%E7%BD%91%E7%BB%9C/"/>
    
    
    <category term="CPU" scheme="http://example.com/tags/CPU/"/>
    
    <category term="GPU" scheme="http://example.com/tags/GPU/"/>
    
    <category term="IB" scheme="http://example.com/tags/IB/"/>
    
    <category term="RDMA" scheme="http://example.com/tags/RDMA/"/>
    
    <category term="RoCEv2" scheme="http://example.com/tags/RoCEv2/"/>
    
    <category term="PCIe" scheme="http://example.com/tags/PCIe/"/>
    
    <category term="智算网络" scheme="http://example.com/tags/%E6%99%BA%E7%AE%97%E7%BD%91%E7%BB%9C/"/>
    
    <category term="Scale up" scheme="http://example.com/tags/Scale-up/"/>
    
    <category term="Scale out" scheme="http://example.com/tags/Scale-out/"/>
    
    <category term="AIGC" scheme="http://example.com/tags/AIGC/"/>
    
  </entry>
  
  <entry>
    <title>GPU | CUDA核心与TensorCore的区别</title>
    <link href="http://example.com/2024/08/08/CUDA%E6%A0%B8%E5%BF%83%E4%B8%8ETensorCore%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
    <id>http://example.com/2024/08/08/CUDA%E6%A0%B8%E5%BF%83%E4%B8%8ETensorCore%E7%9A%84%E5%8C%BA%E5%88%AB/</id>
    <published>2024-08-08T12:45:17.000Z</published>
    <updated>2024-08-08T12:51:05.853Z</updated>
    
    <content type="html"><![CDATA[<p><code>CUDA核心</code>和<code>Tensor Core</code>是<code>NVIDIA GPU</code>中两种不同类型的计算核心且两种核心存在明显的差别，<code>CUDA核心数量</code>和<code>Tensor Core数量</code>是反映GPU计算性能的重要参数，那么CUDA核心与Tensor Core到底是什么？</p><span id="more"></span><p><code>CUDA</code>是<code>NVIDIA</code>发明的<code>并行计算平台</code>和编程模型，<code>CUDA</code>利用<code>GPU</code>强大的<code>并行处理能力</code>提升计算的性能。<code>CUDA核心</code>主要用于执行标准的<code>浮点运算</code>（单精度或双精度），每个<code>CUDA核心</code>每个时钟周期可执行<code>乘加操作</code>，适用于各种<code>通用</code>计算任务。</p><p><code>Tensor Core</code>专为<code>深度学习</code>和<code>AI</code>工作负载设计，用于<code>加速矩阵运算</code>，特别是处理<code>半精度(FP16)</code>和<code>全精度(FP32)</code>的<code>矩阵乘法和累加操作</code>，能够优化<code>深度学习训练和推理</code>过程。第一代<code>Tensor Core</code>是随着<code>Volta</code>架构一起推出的，一代<code>Tensor Core</code>允许两个 4 x 4 FP16 矩阵相乘并添加到一个 4 x 4 FP16 或 FP32 矩阵中（如下图所示），可以实现<code>混合精度训练</code>。</p><p><img src="/2024/08/08/CUDA%E6%A0%B8%E5%BF%83%E4%B8%8ETensorCore%E7%9A%84%E5%8C%BA%E5%88%AB/MAC.png"></p><p>参考链接：<a href="https://developer.volcengine.com/articles/7387624872916353043">一文理解 GPU 张量核心（Tensor Core）</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;code&gt;CUDA核心&lt;/code&gt;和&lt;code&gt;Tensor Core&lt;/code&gt;是&lt;code&gt;NVIDIA GPU&lt;/code&gt;中两种不同类型的计算核心且两种核心存在明显的差别，&lt;code&gt;CUDA核心数量&lt;/code&gt;和&lt;code&gt;Tensor Core数量&lt;/code&gt;是反映GPU计算性能的重要参数，那么CUDA核心与Tensor Core到底是什么？&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    <category term="AI基础设施" scheme="http://example.com/categories/AI/AI%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/"/>
    
    <category term="GPU" scheme="http://example.com/categories/AI/AI%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/GPU/"/>
    
    <category term="CUDA核心与Tensor Core的区别" scheme="http://example.com/categories/AI/AI%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/GPU/CUDA%E6%A0%B8%E5%BF%83%E4%B8%8ETensor-Core%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
    
    
    <category term="GPU" scheme="http://example.com/tags/GPU/"/>
    
    <category term="CUDA" scheme="http://example.com/tags/CUDA/"/>
    
    <category term="Tensor Core" scheme="http://example.com/tags/Tensor-Core/"/>
    
  </entry>
  
  <entry>
    <title>指令集 | 指令集以及国产处理器现状</title>
    <link href="http://example.com/2024/08/08/%E6%8C%87%E4%BB%A4%E9%9B%86%E4%BB%A5%E5%8F%8A%E5%9B%BD%E4%BA%A7%E5%A4%84%E7%90%86%E5%99%A8%E7%8E%B0%E7%8A%B6/"/>
    <id>http://example.com/2024/08/08/%E6%8C%87%E4%BB%A4%E9%9B%86%E4%BB%A5%E5%8F%8A%E5%9B%BD%E4%BA%A7%E5%A4%84%E7%90%86%E5%99%A8%E7%8E%B0%E7%8A%B6/</id>
    <published>2024-08-08T12:40:13.000Z</published>
    <updated>2024-08-29T12:40:40.593Z</updated>
    
    <content type="html"><![CDATA[<ul><li>指令集以及对应的国产处理器<ul><li>CISC<ul><li>X86<ul><li>海光</li><li>兆芯</li></ul></li><li>……</li></ul></li><li>RISC<ul><li>ARM<ul><li>鲲鹏、飞腾、珠峰</li></ul></li><li>RISC-V</li><li>MIPS<ul><li>龙芯 LoongArch</li></ul></li><li>Alpha<ul><li>申威 SW_64</li></ul></li><li>……</li></ul></li></ul></li></ul><span id="more"></span><p>参考链接：<a href="https://mp.weixin.qq.com/s/XA2UtNighbmaAsi2Fua93A">一文读懂面向数据中心的高性能通用RISC-V处理器技术（上）</a></p>]]></content>
    
    
    <summary type="html">&lt;ul&gt;
&lt;li&gt;指令集以及对应的国产处理器&lt;ul&gt;
&lt;li&gt;CISC&lt;ul&gt;
&lt;li&gt;X86&lt;ul&gt;
&lt;li&gt;海光&lt;/li&gt;
&lt;li&gt;兆芯&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;……&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;RISC&lt;ul&gt;
&lt;li&gt;ARM&lt;ul&gt;
&lt;li&gt;鲲鹏、飞腾、珠峰&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;RISC-V&lt;/li&gt;
&lt;li&gt;MIPS&lt;ul&gt;
&lt;li&gt;龙芯 LoongArch&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Alpha&lt;ul&gt;
&lt;li&gt;申威 SW_64&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;……&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    <category term="AI基础设施" scheme="http://example.com/categories/AI/AI%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/"/>
    
    <category term="CPU" scheme="http://example.com/categories/AI/AI%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/CPU/"/>
    
    <category term="指令集" scheme="http://example.com/categories/AI/AI%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/CPU/%E6%8C%87%E4%BB%A4%E9%9B%86/"/>
    
    <category term="指令集以及国产处理器现状" scheme="http://example.com/categories/AI/AI%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/CPU/%E6%8C%87%E4%BB%A4%E9%9B%86/%E6%8C%87%E4%BB%A4%E9%9B%86%E4%BB%A5%E5%8F%8A%E5%9B%BD%E4%BA%A7%E5%A4%84%E7%90%86%E5%99%A8%E7%8E%B0%E7%8A%B6/"/>
    
    
    <category term="指令集" scheme="http://example.com/tags/%E6%8C%87%E4%BB%A4%E9%9B%86/"/>
    
    <category term="CISC" scheme="http://example.com/tags/CISC/"/>
    
    <category term="RISC" scheme="http://example.com/tags/RISC/"/>
    
    <category term="X86" scheme="http://example.com/tags/X86/"/>
    
    <category term="ARM" scheme="http://example.com/tags/ARM/"/>
    
    <category term="国产处理器" scheme="http://example.com/tags/%E5%9B%BD%E4%BA%A7%E5%A4%84%E7%90%86%E5%99%A8/"/>
    
    <category term="RISC-V" scheme="http://example.com/tags/RISC-V/"/>
    
  </entry>
  
  <entry>
    <title>后端优化 | 循环优化</title>
    <link href="http://example.com/2024/07/22/%E5%BE%AA%E7%8E%AF%E4%BC%98%E5%8C%96/"/>
    <id>http://example.com/2024/07/22/%E5%BE%AA%E7%8E%AF%E4%BC%98%E5%8C%96/</id>
    <published>2024-07-22T15:23:23.000Z</published>
    <updated>2024-07-22T15:30:28.594Z</updated>
    
    <content type="html"><![CDATA[<p>采用深度学习编译器对深度学习代码进行编译时，在编译器后端会对IR代码进行后端优化，循环优化就包括在后端优化中，后端优化能够加速代码的运行效率。深度学习编译器编译流程如下图所示：</p><span id="more"></span><p><img src="/2024/07/22/%E5%BE%AA%E7%8E%AF%E4%BC%98%E5%8C%96/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E5%99%A8%E7%BC%96%E8%AF%91%E6%B5%81%E7%A8%8B.png"></p><p>循环优化方式：</p><ul><li><p>循环融合（loop fusion）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sayHello</span>():</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;hello&quot;</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sayBye</span>():</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;bye&quot;</span>)</span><br><span class="line"><span class="comment"># 融合前</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000000</span>):</span><br><span class="line">    sayHello()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000000</span>):</span><br><span class="line">    sayBye()</span><br><span class="line"><span class="comment"># 融合后（将两个循环融合为一个）</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000000</span>):</span><br><span class="line">    sayHello()</span><br><span class="line">    sayBye()</span><br></pre></td></tr></table></figure></li><li><p>循环重新排序（loop reorder）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 重排序前</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000000</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"><span class="comment"># 重排序后（采用迭代次数较小的循环驱动内层迭代次数较大的循环能减少内存的消耗）</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000000</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure></li><li><p>循环展开（loop unrolling）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 展开前</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    sayHello()</span><br><span class="line"><span class="comment"># 展开后</span></span><br><span class="line">sayHello()</span><br><span class="line">sayHello()</span><br><span class="line">sayHello()</span><br><span class="line">sayHello()</span><br><span class="line">sayHello()</span><br><span class="line">sayHello()</span><br><span class="line">sayHello()</span><br><span class="line">sayHello()</span><br><span class="line">sayHello()</span><br><span class="line">sayHello()</span><br></pre></td></tr></table></figure></li><li><p>循环分块（loop tiling）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sum_2d_array</span>(<span class="params">n, A</span>):</span></span><br><span class="line">    <span class="built_in">sum</span> = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            <span class="built_in">sum</span> += A[i][j]</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sum_2d_array</span>(<span class="params">n, A</span>) &#123;</span></span><br><span class="line"><span class="function">    <span class="title">sum</span> = 0</span></span><br><span class="line"><span class="function">    <span class="title">block_size</span> = 8</span></span><br><span class="line"><span class="function">    <span class="title">for</span> <span class="title">i</span> <span class="title">in</span> <span class="title">range</span>(<span class="params"><span class="number">0</span>, n, block_size</span>):</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, n, block_size):</span><br><span class="line">    <span class="keyword">for</span> bi <span class="keyword">in</span> <span class="built_in">range</span>(i, i + block_size):</span><br><span class="line">    <span class="keyword">for</span> bj <span class="keyword">in</span> <span class="built_in">range</span>(j, j + block_size):</span><br><span class="line">    <span class="built_in">sum</span> += A[bi][bj]</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;采用深度学习编译器对深度学习代码进行编译时，在编译器后端会对IR代码进行后端优化，循环优化就包括在后端优化中，后端优化能够加速代码的运行效率。深度学习编译器编译流程如下图所示：&lt;/p&gt;</summary>
    
    
    
    <category term="编译器" scheme="http://example.com/categories/%E7%BC%96%E8%AF%91%E5%99%A8/"/>
    
    <category term="深度学习编译器" scheme="http://example.com/categories/%E7%BC%96%E8%AF%91%E5%99%A8/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E5%99%A8/"/>
    
    <category term="后端优化" scheme="http://example.com/categories/%E7%BC%96%E8%AF%91%E5%99%A8/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E5%99%A8/%E5%90%8E%E7%AB%AF%E4%BC%98%E5%8C%96/"/>
    
    <category term="循环优化" scheme="http://example.com/categories/%E7%BC%96%E8%AF%91%E5%99%A8/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E5%99%A8/%E5%90%8E%E7%AB%AF%E4%BC%98%E5%8C%96/%E5%BE%AA%E7%8E%AF%E4%BC%98%E5%8C%96/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="编译器" scheme="http://example.com/tags/%E7%BC%96%E8%AF%91%E5%99%A8/"/>
    
    <category term="后端优化" scheme="http://example.com/tags/%E5%90%8E%E7%AB%AF%E4%BC%98%E5%8C%96/"/>
    
    <category term="循环优化" scheme="http://example.com/tags/%E5%BE%AA%E7%8E%AF%E4%BC%98%E5%8C%96/"/>
    
    <category term="循环融合" scheme="http://example.com/tags/%E5%BE%AA%E7%8E%AF%E8%9E%8D%E5%90%88/"/>
    
    <category term="循环重排序" scheme="http://example.com/tags/%E5%BE%AA%E7%8E%AF%E9%87%8D%E6%8E%92%E5%BA%8F/"/>
    
    <category term="循环展开" scheme="http://example.com/tags/%E5%BE%AA%E7%8E%AF%E5%B1%95%E5%BC%80/"/>
    
    <category term="循环分块" scheme="http://example.com/tags/%E5%BE%AA%E7%8E%AF%E5%88%86%E5%9D%97/"/>
    
  </entry>
  
  <entry>
    <title>RDMA | IB与RoCEv2的对比</title>
    <link href="http://example.com/2024/07/22/IB%E4%B8%8ERDMA%E7%9A%84%E5%AF%B9%E6%AF%94/"/>
    <id>http://example.com/2024/07/22/IB%E4%B8%8ERDMA%E7%9A%84%E5%AF%B9%E6%AF%94/</id>
    <published>2024-07-22T15:22:55.000Z</published>
    <updated>2024-07-22T15:27:11.741Z</updated>
    
    <content type="html"><![CDATA[<p>Nvidia Infiniband 与 RoCEv2的对比：</p><p><img src="/2024/07/22/IB%E4%B8%8ERDMA%E7%9A%84%E5%AF%B9%E6%AF%94/IB%E4%B8%8ERDMA%E7%9A%84%E5%AF%B9%E6%AF%94.png"></p><p>参考链接：<a href="https://mp.weixin.qq.com/s/kzcrq9ycET_K7TrIQT_nSg">Infiniband和RoCEv2，以及RDMA技术的未来</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Nvidia Infiniband 与 RoCEv2的对比：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2024/07/22/IB%E4%B8%8ERDMA%E7%9A%84%E5%AF%B9%E6%AF%94/IB%E4%B8%8ERDMA%E7%9A%84%E5%AF%B9%</summary>
      
    
    
    
    <category term="计算机基础" scheme="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"/>
    
    <category term="计算机网络" scheme="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    <category term="RDMA" scheme="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/RDMA/"/>
    
    <category term="IB与RoCEv2的对比" scheme="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/RDMA/IB%E4%B8%8ERoCEv2%E7%9A%84%E5%AF%B9%E6%AF%94/"/>
    
    
    <category term="IB" scheme="http://example.com/tags/IB/"/>
    
    <category term="RDMA" scheme="http://example.com/tags/RDMA/"/>
    
    <category term="RoCEv2" scheme="http://example.com/tags/RoCEv2/"/>
    
  </entry>
  
  <entry>
    <title>TVM | TVM介绍</title>
    <link href="http://example.com/2024/07/22/TVM%E4%BB%8B%E7%BB%8D/"/>
    <id>http://example.com/2024/07/22/TVM%E4%BB%8B%E7%BB%8D/</id>
    <published>2024-07-22T15:22:16.000Z</published>
    <updated>2024-07-22T15:25:11.708Z</updated>
    
    <content type="html"><![CDATA[<p>参考文章：<a href="https://www.zhihu.com/question/532085071/answer/3154629417">(38 封私信 / 80 条消息) 深度学习编译器研发工程师的工作主要是集中于编译技术的前端、中端还是后端？ - 知乎 (zhihu.com)</a></p><p>参考视频：<a href="https://www.bilibili.com/video/BV1u6421M7jN/?spm_id_from=333.788&vd_source=0d5e0d352ee1cac3b12442c119f31bfc">【3rd-party】20240215 深度学习编译技术及TVM实践分享_哔哩哔哩_bilibili</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;参考文章：&lt;a href=&quot;https://www.zhihu.com/question/532085071/answer/3154629417&quot;&gt;(38 封私信 / 80 条消息) 深度学习编译器研发工程师的工作主要是集中于编译技术的前端、中端还是后端？ - 知乎 (zh</summary>
      
    
    
    
    <category term="编译器" scheme="http://example.com/categories/%E7%BC%96%E8%AF%91%E5%99%A8/"/>
    
    <category term="深度学习编译器" scheme="http://example.com/categories/%E7%BC%96%E8%AF%91%E5%99%A8/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E5%99%A8/"/>
    
    <category term="TVM" scheme="http://example.com/categories/%E7%BC%96%E8%AF%91%E5%99%A8/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E5%99%A8/TVM/"/>
    
    <category term="TVM介绍" scheme="http://example.com/categories/%E7%BC%96%E8%AF%91%E5%99%A8/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E5%99%A8/TVM/TVM%E4%BB%8B%E7%BB%8D/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="编译器" scheme="http://example.com/tags/%E7%BC%96%E8%AF%91%E5%99%A8/"/>
    
    <category term="TVM" scheme="http://example.com/tags/TVM/"/>
    
  </entry>
  
  <entry>
    <title>CUDA | CUDA的新竞争者</title>
    <link href="http://example.com/2024/07/19/CUDA%E7%9A%84%E6%96%B0%E7%AB%9E%E4%BA%89%E8%80%85/"/>
    <id>http://example.com/2024/07/19/CUDA%E7%9A%84%E6%96%B0%E7%AB%9E%E4%BA%89%E8%80%85/</id>
    <published>2024-07-19T15:23:07.000Z</published>
    <updated>2024-07-19T15:34:11.360Z</updated>
    
    <content type="html"><![CDATA[<p>AI时代，Nvidia作为HPC的头号玩家，其手中的主要利器有：高算力GPU、高速互联设备、CUDA，其中CUDA可以称之为Nvidia的护城河，只有使用CUDA才能利用Nvidia GPU进行高效的运行AI算法。</p><span id="more"></span><p>2024.7.12 英国公司Spectral Compute发布了<a href="[SCALE GPGPU Programming Language (scale-lang.com)](https://scale-lang.com/posts/2024-07-12-release-announcement)">SCALE BETA</a>，意图突破Nvidia的护城河-CUDA。SCALE是一个GPGPU工具链，它允许CUDA程序原生地运行在AMD GPUs上，后期也会提供其他厂商GPU的支持。SCALE的开发主要是为了让用户能够自由地使用GPGPU编程工具和GPU硬件，这些工具和硬件最好地满足了用户的开发需求。SCALE的横空出世，使得“Write once, run anywhere”对GPU来说称为可能。Spectral Compute公司计划通过跨越CUDA编程语言和其他厂商硬件之间的兼容性差距来实现“Write once, run anywhere”。</p><p>SCALE是一个类似于Nvidia CUDA工具的GPGPU工具，该工具能够将CUDA代码编译成面向非Nvidia GPUs的二进制代码。SCALE旨在与CUDA源代码兼容，包括支持内联PTX和NVCC的C++特性。</p><p>SCALE工具允许在其他厂商的GPU上运行CUDA code，打破必须使用与CUDA绑定的Nvidia硬件，这促进了AI算法开发以及部署的灵活度，可根据市场GPU的存量以及自身经济能力灵活的选择算力设备。</p><p>SCALE 文档：<a href="https://docs.scale-lang.com/examples/">SCALE Example Programs - SCALE documentation (scale-lang.com)</a></p><p>参考链接：<a href="https://mp.weixin.qq.com/s/y2M0bqTMzWC6F4sOkw2yHw">突破CUDA包围圈，再出一招</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;AI时代，Nvidia作为HPC的头号玩家，其手中的主要利器有：高算力GPU、高速互联设备、CUDA，其中CUDA可以称之为Nvidia的护城河，只有使用CUDA才能利用Nvidia GPU进行高效的运行AI算法。&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    <category term="AI工具链" scheme="http://example.com/categories/AI/AI%E5%B7%A5%E5%85%B7%E9%93%BE/"/>
    
    <category term="CUDA" scheme="http://example.com/categories/AI/AI%E5%B7%A5%E5%85%B7%E9%93%BE/CUDA/"/>
    
    <category term="CUDA新的竞争者" scheme="http://example.com/categories/AI/AI%E5%B7%A5%E5%85%B7%E9%93%BE/CUDA/CUDA%E6%96%B0%E7%9A%84%E7%AB%9E%E4%BA%89%E8%80%85/"/>
    
    
    <category term="AI" scheme="http://example.com/tags/AI/"/>
    
    <category term="GPU" scheme="http://example.com/tags/GPU/"/>
    
    <category term="CUDA" scheme="http://example.com/tags/CUDA/"/>
    
    <category term="Nvidia" scheme="http://example.com/tags/Nvidia/"/>
    
    <category term="SCALE" scheme="http://example.com/tags/SCALE/"/>
    
    <category term="HPC" scheme="http://example.com/tags/HPC/"/>
    
  </entry>
  
  <entry>
    <title>AI基础设施 | 什么是智算中心</title>
    <link href="http://example.com/2024/07/19/%E4%BB%80%E4%B9%88%E6%98%AF%E6%99%BA%E7%AE%97%E4%B8%AD%E5%BF%83/"/>
    <id>http://example.com/2024/07/19/%E4%BB%80%E4%B9%88%E6%98%AF%E6%99%BA%E7%AE%97%E4%B8%AD%E5%BF%83/</id>
    <published>2024-07-19T15:22:56.000Z</published>
    <updated>2024-07-19T15:37:22.379Z</updated>
    
    <content type="html"><![CDATA[<ol><li>三种数据中心<ul><li>通算中心（通用服务器-以CPU为主要芯片）</li><li>智算中心（智算服务器-以GPU/NPU/TPU等加速芯片为主）</li><li>超算中心（超级计算机）</li></ul></li></ol><span id="more"></span><ol start="2"><li><p>为什么要有智算中心？</p><ul><li><p>应用类型变化：传统应用以web应用为主，部署在以CPU为核心算力的通用服务器上。随着AI的快速发展，AI Native类型的应用快速占领市场，AI Native应用需要更多的算力。</p></li><li><p>传统服务器算力不足：大模型、其他AI算法的训练、推理过程需要更大的算力支撑，传统的通用服务器算力不能满足模型的训练和推理，因此需要构建拥有强大算力、高带宽通信的智算中心。</p></li></ul></li></ol><ol start="3"><li><p>什么是智算中心？</p><p><code>智算中心</code>由智算服务器组成，是以<code>人工智能</code>计算任务为主的<code>数据中心</code>。智算中心采用专门的AI算力硬件（<code>GPU</code>/<code>NPU</code>/<code>TPU</code>），适合高效运行AI算法，可以用于计算机视觉（Computer Vision）、自然语言处理（Natural Language Processing）、机器学习（Machine Learning）等领域，处理图像识别（Image Recognition）、语音识别（Speech Recognition）、文本分析（Text Analysis）、模型训练推理（Model Training and Inferring）等任务。</p></li></ol><ol start="4"><li><p>智算中心的核心-智算服务器：</p><ul><li><p>训练服务器（AI算力板卡多于推理服务器）：用于AI模型训练</p></li><li><p>推理服务器：用于AI算法推理</p></li><li><p>训推一体服务器：用于AI算法的训练和推理</p></li><li><p>算力大小：训练服务器 &gt;= 训推一体服务器 &gt; 推理服务器</p></li></ul></li></ol><p>参考链接：<a href="https://mp.weixin.qq.com/s/pbxWZvnRF1BMLQlwADxsNw">四问四答，彻底看懂智算中心！</a></p>]]></content>
    
    
    <summary type="html">&lt;ol&gt;
&lt;li&gt;三种数据中心&lt;ul&gt;
&lt;li&gt;通算中心（通用服务器-以CPU为主要芯片）&lt;/li&gt;
&lt;li&gt;智算中心（智算服务器-以GPU/NPU/TPU等加速芯片为主）&lt;/li&gt;
&lt;li&gt;超算中心（超级计算机）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    <category term="AI基础设施" scheme="http://example.com/categories/AI/AI%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/"/>
    
    <category term="智算中心" scheme="http://example.com/categories/AI/AI%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/%E6%99%BA%E7%AE%97%E4%B8%AD%E5%BF%83/"/>
    
    <category term="什么是智算中心？" scheme="http://example.com/categories/AI/AI%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/%E6%99%BA%E7%AE%97%E4%B8%AD%E5%BF%83/%E4%BB%80%E4%B9%88%E6%98%AF%E6%99%BA%E7%AE%97%E4%B8%AD%E5%BF%83%EF%BC%9F/"/>
    
    
    <category term="AI" scheme="http://example.com/tags/AI/"/>
    
    <category term="GPU" scheme="http://example.com/tags/GPU/"/>
    
    <category term="智算中心" scheme="http://example.com/tags/%E6%99%BA%E7%AE%97%E4%B8%AD%E5%BF%83/"/>
    
    <category term="智算服务器" scheme="http://example.com/tags/%E6%99%BA%E7%AE%97%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
    
  </entry>
  
  <entry>
    <title>VPN | 什么是VPN</title>
    <link href="http://example.com/2024/07/19/%E4%BB%80%E4%B9%88%E6%98%AFVPN/"/>
    <id>http://example.com/2024/07/19/%E4%BB%80%E4%B9%88%E6%98%AFVPN/</id>
    <published>2024-07-19T15:22:34.000Z</published>
    <updated>2024-07-19T15:30:56.892Z</updated>
    
    <content type="html"><![CDATA[<p>在工作、或学习中，如果连接的是内部网络，则可以直接访问内部网络资源，如果在家或出差时想要访问内部网络资源，常需要通过<code>VPN</code>才能访问公司/学校<code>内部网络资源</code>。那么VPN到底是什么呢？</p><span id="more"></span><p>VPN（Virtual Private Network）: 一种利用<code>公共网络</code>建立<code>专用网络</code>的技术。通过加密和隧道技术，VPN可以保护<code>数据传输的安全性和隐私性</code>，同时能够使<code>远程</code>用户安全访问企业/学校的内部网络资源。 </p><p>VPN的应用场景：</p><ol><li>远程工作</li><li>数据保护</li><li>隐私保护</li></ol><p>工作原理：</p><p>VPN通过创建一个加密的隧道来传输数据，确保数据在传输过程中的安全和隐私。VPN用户连接到VPN服务器后，所有网络流量都会被加密并通过隧道传输到VPN服务器，然后再解密和转发到互联网或内部网络</p><p><img src="/2024/07/19/%E4%BB%80%E4%B9%88%E6%98%AFVPN/VPN.png"></p><p>VPN分类以及区别：</p><ul><li>IPSec VPN(Internet Protocol Security VPN，互联网协议安全VPN)</li><li>SSL/TLS VPN（SSL-VPN,Secure Socket Layer, 安全套接层VPN）</li></ul><p>![](./什么是VPN/two types VPN.png)</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;在工作、或学习中，如果连接的是内部网络，则可以直接访问内部网络资源，如果在家或出差时想要访问内部网络资源，常需要通过&lt;code&gt;VPN&lt;/code&gt;才能访问公司/学校&lt;code&gt;内部网络资源&lt;/code&gt;。那么VPN到底是什么呢？&lt;/p&gt;</summary>
    
    
    
    <category term="计算机基础" scheme="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"/>
    
    <category term="计算机网络" scheme="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    <category term="VPN" scheme="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/VPN/"/>
    
    <category term="什么是VPN？" scheme="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/VPN/%E4%BB%80%E4%B9%88%E6%98%AFVPN%EF%BC%9F/"/>
    
    
    <category term="计算机网络" scheme="http://example.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    <category term="VPN" scheme="http://example.com/tags/VPN/"/>
    
    <category term="SSL VPN" scheme="http://example.com/tags/SSL-VPN/"/>
    
    <category term="IPsec VPN" scheme="http://example.com/tags/IPsec-VPN/"/>
    
  </entry>
  
  <entry>
    <title>TCP/IP | 秒懂TCPIP</title>
    <link href="http://example.com/2024/07/19/%E7%A7%92%E6%87%82TCPIP/"/>
    <id>http://example.com/2024/07/19/%E7%A7%92%E6%87%82TCPIP/</id>
    <published>2024-07-19T15:22:22.000Z</published>
    <updated>2024-07-19T15:25:13.411Z</updated>
    
    <content type="html"><![CDATA[<p>数据包传输过程中都用到了哪些协议？</p><p><a href="https://mp.weixin.qq.com/s/dysSZLyTRaSbr4650aQ7uA">全网介绍TCP/IP最全的文章</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;数据包传输过程中都用到了哪些协议？&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s/dysSZLyTRaSbr4650aQ7uA&quot;&gt;全网介绍TCP/IP最全的文章&lt;/a&gt;&lt;/p&gt;
</summary>
      
    
    
    
    <category term="计算机基础" scheme="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"/>
    
    <category term="计算机网络" scheme="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    <category term="TCP/IP" scheme="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-IP/"/>
    
    <category term="秒懂TCPIP" scheme="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-IP/%E7%A7%92%E6%87%82TCPIP/"/>
    
    
    <category term="计算机网络" scheme="http://example.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    <category term="TCP/IP" scheme="http://example.com/tags/TCP-IP/"/>
    
    <category term="数据包" scheme="http://example.com/tags/%E6%95%B0%E6%8D%AE%E5%8C%85/"/>
    
    <category term="协议" scheme="http://example.com/tags/%E5%8D%8F%E8%AE%AE/"/>
    
  </entry>
  
  <entry>
    <title>分布式训练集合通信以及集合通信原语</title>
    <link href="http://example.com/2024/07/17/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E9%9B%86%E5%90%88%E9%80%9A%E4%BF%A1%E4%BB%A5%E5%8F%8A%E9%9B%86%E5%90%88%E9%80%9A%E4%BF%A1%E5%8E%9F%E8%AF%AD/"/>
    <id>http://example.com/2024/07/17/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E9%9B%86%E5%90%88%E9%80%9A%E4%BF%A1%E4%BB%A5%E5%8F%8A%E9%9B%86%E5%90%88%E9%80%9A%E4%BF%A1%E5%8E%9F%E8%AF%AD/</id>
    <published>2024-07-17T12:09:13.000Z</published>
    <updated>2024-07-17T12:10:17.217Z</updated>
    
    <content type="html"><![CDATA[<p><code>大模型</code>的训练需要用到多个配有<code>GPU</code>的节点，GPU间通过<code>集合通信原语</code>进行通信，从而实现<code>GPU</code>间的<code>数据交换</code>和<code>共享</code>。</p><span id="more"></span><p><code>通信原语</code>的具体内容参考：<a href="https://zhuanlan.zhihu.com/p/493092647">分布式训练 – 第3篇 - 分布式训练常用的集合通信及其通信原语 - 知乎 (zhihu.com)</a>，这篇文章分析、总结的非常到位，此处不再额外总结。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;code&gt;大模型&lt;/code&gt;的训练需要用到多个配有&lt;code&gt;GPU&lt;/code&gt;的节点，GPU间通过&lt;code&gt;集合通信原语&lt;/code&gt;进行通信，从而实现&lt;code&gt;GPU&lt;/code&gt;间的&lt;code&gt;数据交换&lt;/code&gt;和&lt;code&gt;共享&lt;/code&gt;。&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    <category term="深度学习" scheme="http://example.com/categories/AI/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="大模型" scheme="http://example.com/categories/AI/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="分布式训练" scheme="http://example.com/categories/AI/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/"/>
    
    <category term="集合通信以及集合通信原语" scheme="http://example.com/categories/AI/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%A8%A1%E5%9E%8B/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/%E9%9B%86%E5%90%88%E9%80%9A%E4%BF%A1%E4%BB%A5%E5%8F%8A%E9%9B%86%E5%90%88%E9%80%9A%E4%BF%A1%E5%8E%9F%E8%AF%AD/"/>
    
    
    <category term="GPU" scheme="http://example.com/tags/GPU/"/>
    
    <category term="大模型" scheme="http://example.com/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="通信原语" scheme="http://example.com/tags/%E9%80%9A%E4%BF%A1%E5%8E%9F%E8%AF%AD/"/>
    
    <category term="集合通信" scheme="http://example.com/tags/%E9%9B%86%E5%90%88%E9%80%9A%E4%BF%A1/"/>
    
  </entry>
  
</feed>
