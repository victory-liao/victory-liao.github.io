<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>victory的博客</title>
  
  <subtitle>长安一片月，万户捣衣声</subtitle>
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2025-06-16T13:04:58.314Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>victory-liao</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>python | 定时任务</title>
    <link href="http://example.com/2025/06/16/%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/"/>
    <id>http://example.com/2025/06/16/%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/</id>
    <published>2025-06-16T13:01:19.000Z</published>
    <updated>2025-06-16T13:04:58.314Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Python定时任务调度"><a href="#Python定时任务调度" class="headerlink" title="Python定时任务调度"></a>Python定时任务调度</h1><p>定时任务调度（<strong>Scheduled Task Execution</strong>）是一种在预定时间或周期性地自动执行特定操作的机制。它广泛应用于自动化运维、数据处理、日志清理、监控告警、定时推送等场景。</p><hr><h3 id="🕰️-定时任务调度的核心概念"><a href="#🕰️-定时任务调度的核心概念" class="headerlink" title="🕰️ 定时任务调度的核心概念"></a>🕰️ 定时任务调度的核心概念</h3><table><thead><tr><th>概念</th><th>说明</th></tr></thead><tbody><tr><td><strong>任务（Task）</strong></td><td>要执行的具体操作，通常是一个函数或命令。</td></tr><tr><td><strong>调度器（Scheduler）</strong></td><td>管理和触发任务执行的组件。</td></tr><tr><td><strong>触发条件（Trigger）</strong></td><td>决定任务何时执行的规则，如：固定间隔、延迟执行、cron 表达式等。</td></tr><tr><td><strong>执行方式</strong></td><td>可同步或异步执行任务。</td></tr></tbody></table><hr><h3 id="📌-定时任务的常见类型"><a href="#📌-定时任务的常见类型" class="headerlink" title="📌 定时任务的常见类型"></a>📌 定时任务的常见类型</h3><ol><li><strong>一次性任务（One-time Task）</strong><ul><li>执行一次后自动结束。</li><li>示例：5 秒后发送一条通知。</li></ul></li><li><strong>周期性任务（Recurring Task）</strong><ul><li>按照固定间隔重复执行。</li><li>示例：每 5 分钟检查服务器状态。</li></ul></li><li><strong>延迟首次执行任务（Delayed First Execution）</strong><ul><li>首次执行有延迟，之后按周期执行。</li><li>示例：启动后等待 10 秒再开始心跳检测。</li></ul></li><li><strong>基于时间表达式的任务（Cron-like Task）</strong><ul><li>使用类似 <code>cron</code> 的语法定义执行时间点。</li><li>示例：每天凌晨 2:00 执行数据库备份。</li></ul></li></ol><hr><h3 id="✅-定时任务调度的应用场景"><a href="#✅-定时任务调度的应用场景" class="headerlink" title="✅ 定时任务调度的应用场景"></a>✅ 定时任务调度的应用场景</h3><table><thead><tr><th>场景</th><th>描述</th></tr></thead><tbody><tr><td>数据同步</td><td>每小时从远程服务器拉取最新数据</td></tr><tr><td>日志清理</td><td>每天凌晨删除过期日志文件</td></tr><tr><td>报表生成</td><td>每月第一天自动生成统计报表</td></tr><tr><td>健康检查</td><td>每隔一段时间检测服务可用性</td></tr><tr><td>自动提醒</td><td>每天上午9点推送待办事项</td></tr></tbody></table><hr><h3 id="🧩-定时任务调度的实现方式（Python-中）"><a href="#🧩-定时任务调度的实现方式（Python-中）" class="headerlink" title="🧩 定时任务调度的实现方式（Python 中）"></a>🧩 定时任务调度的实现方式（Python 中）</h3><h4 id="1-简单轮询-sleep"><a href="#1-简单轮询-sleep" class="headerlink" title="1. 简单轮询 + sleep"></a>1. <strong>简单轮询 + sleep</strong></h4><ul><li>如你提供的 <a href="file://C:\Users\victory\Desktop\cron-job.py#L9-L105">SimpleCronJob</a> 类所示。</li><li>利用 <code>while</code> 循环 + <code>time.sleep()</code> 实现秒级轮询。</li><li>适用于轻量级任务。</li></ul><h4 id="2-使用第三方库"><a href="#2-使用第三方库" class="headerlink" title="2. 使用第三方库"></a>2. <strong>使用第三方库</strong></h4><ul><li><strong>APScheduler</strong>：功能强大的任务调度器，支持 cron、日期、间隔三种触发器。</li><li><strong>Celery Beat</strong>：分布式任务调度系统，适合大型项目。</li><li><strong>schedule</strong>：简洁易用的库，适合脚本化任务。</li></ul><h4 id="3-操作系统级定时任务"><a href="#3-操作系统级定时任务" class="headerlink" title="3. 操作系统级定时任务"></a>3. <strong>操作系统级定时任务</strong></h4><ul><li>Linux 下使用 <code>crontab</code>。</li><li>Windows 下使用“任务计划程序”。</li></ul><hr><h3 id="🔁-示例对比：不同方式实现每-5-秒打印一次"><a href="#🔁-示例对比：不同方式实现每-5-秒打印一次" class="headerlink" title="🔁 示例对比：不同方式实现每 5 秒打印一次"></a>🔁 示例对比：不同方式实现每 5 秒打印一次</h3><h4 id="方式一：原生-Python（当前类）"><a href="#方式一：原生-Python（当前类）" class="headerlink" title="方式一：原生 Python（当前类）"></a>方式一：原生 Python（当前类）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cron.add_task(<span class="string">&quot;periodic_task&quot;</span>, <span class="keyword">lambda</span>: <span class="built_in">print</span>(<span class="string">&quot;🔁 周期性任务执行&quot;</span>), interval_seconds=<span class="number">5</span>)</span><br></pre></td></tr></table></figure><h4 id="方式二：使用-schedule-库"><a href="#方式二：使用-schedule-库" class="headerlink" title="方式二：使用 schedule 库"></a>方式二：使用 <code>schedule</code> 库</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> schedule</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">job</span>():</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Scheduled task executed&quot;</span>)</span><br><span class="line"></span><br><span class="line">schedule.every(<span class="number">5</span>).seconds.do(job)</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    schedule.run_pending()</span><br><span class="line">    time.sleep(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h4 id="方式三：使用-APScheduler"><a href="#方式三：使用-APScheduler" class="headerlink" title="方式三：使用 APScheduler"></a>方式三：使用 <code>APScheduler</code></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> apscheduler.schedulers.blocking <span class="keyword">import</span> BlockingScheduler</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">job</span>():</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;APScheduled task executed&quot;</span>)</span><br><span class="line"></span><br><span class="line">sched = BlockingScheduler()</span><br><span class="line">sched.add_job(job, <span class="string">&#x27;interval&#x27;</span>, seconds=<span class="number">5</span>)</span><br><span class="line">sched.start()</span><br></pre></td></tr></table></figure><hr><h3 id="🧠-小结"><a href="#🧠-小结" class="headerlink" title="🧠 小结"></a>🧠 小结</h3><table><thead><tr><th>特性</th><th>当前类 <a href="file://C:\Users\victory\Desktop\cron-job.py#L9-L105">SimpleCronJob</a></th><th>APScheduler</th><th>Celery Beat</th><th>crontab</th></tr></thead><tbody><tr><td>易用性</td><td>✅ 简单直接</td><td>✅ 支持多种触发器</td><td>⚠️ 分布式复杂</td><td>✅ 系统级配置</td></tr><tr><td>功能丰富度</td><td>❌ 较基础</td><td>✅ 强大</td><td>✅ 极其强大</td><td>⚠️ 仅基本调度</td></tr><tr><td>多线程支持</td><td>✅ 已封装</td><td>✅ 支持</td><td>✅ 支持</td><td>❌ 单进程</td></tr><tr><td>持久化</td><td>❌ 不支持</td><td>✅ 可持久化</td><td>✅ 支持</td><td>❌ 不支持</td></tr><tr><td>适用场景</td><td>本地测试、小型脚本</td><td>中型应用</td><td>大型分布式系统</td><td>系统维护任务</td></tr></tbody></table><hr><h3 id="📎-总结一句话："><a href="#📎-总结一句话：" class="headerlink" title="📎 总结一句话："></a>📎 总结一句话：</h3><blockquote><p><strong>定时任务调度就是让程序在指定时间或条件下自动执行某些操作，从而实现自动化流程管理。</strong></p></blockquote><h1 id="python定时任务调度实现"><a href="#python定时任务调度实现" class="headerlink" title="python定时任务调度实现"></a>python定时任务调度实现</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Dict</span>, <span class="type">Optional</span>, <span class="type">Callable</span>, <span class="type">Tuple</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleCronJob</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.tasks: <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Tuple</span>[datetime.datetime, datetime.timedelta, <span class="built_in">bool</span>, <span class="built_in">bool</span>, <span class="type">Callable</span>]] = &#123;&#125;</span><br><span class="line">        self.lock = threading.Lock()</span><br><span class="line">        self.stop_flag = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_task</span>(<span class="params">self, task_id: <span class="built_in">str</span>, func: <span class="type">Callable</span>, delay_seconds: <span class="built_in">int</span> = <span class="number">0</span>, interval_seconds: <span class="built_in">int</span> = <span class="number">0</span>, once: <span class="built_in">bool</span> = <span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        添加一个定时任务。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param task_id: 任务唯一标识符</span></span><br><span class="line"><span class="string">        :param func: 要执行的函数</span></span><br><span class="line"><span class="string">        :param delay_seconds: 首次执行延迟时间（秒）</span></span><br><span class="line"><span class="string">        :param interval_seconds: 周期执行间隔时间（秒）</span></span><br><span class="line"><span class="string">        :param once: 是否为一次性任务</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">with</span> self.lock:</span><br><span class="line">            self.tasks[task_id] = (</span><br><span class="line">                <span class="literal">None</span>,  <span class="comment"># last_time</span></span><br><span class="line">                datetime.timedelta(seconds=delay_seconds),</span><br><span class="line">                <span class="literal">False</span>,  <span class="comment"># first_executed</span></span><br><span class="line">                datetime.timedelta(seconds=interval_seconds),</span><br><span class="line">                once,</span><br><span class="line">                func</span><br><span class="line">            )</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;任务 <span class="subst">&#123;task_id&#125;</span> 已注册&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;开始执行定时任务服务...&quot;</span>)</span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> self.stop_flag:</span><br><span class="line">            now = datetime.datetime.now()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">with</span> self.lock:</span><br><span class="line">                <span class="keyword">for</span> task_id, (last_time, delay_time, first_executed, interval_time, once, func) <span class="keyword">in</span> <span class="built_in">list</span>(self.tasks.items()):</span><br><span class="line">                    <span class="keyword">if</span> delay_time.total_seconds() &gt; <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> first_executed:</span><br><span class="line">                        <span class="keyword">if</span> last_time <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                            self.tasks[task_id] = (now, delay_time, first_executed, interval_time, once, func)</span><br><span class="line">                            <span class="keyword">continue</span></span><br><span class="line">                        <span class="keyword">elif</span> (now - last_time) &lt; delay_time:</span><br><span class="line">                            <span class="keyword">continue</span></span><br><span class="line">                        <span class="keyword">else</span>:</span><br><span class="line">                            self.tasks[task_id] = (now, delay_time, <span class="literal">True</span>, interval_time, once, func)</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">elif</span> last_time <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                        self.tasks[task_id] = (now, delay_time, first_executed, interval_time, once, func)</span><br><span class="line">                        <span class="keyword">continue</span></span><br><span class="line">                    <span class="keyword">elif</span> (now - last_time) &lt; interval_time:</span><br><span class="line">                        <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">                    <span class="comment"># 执行任务</span></span><br><span class="line">                    <span class="keyword">try</span>:</span><br><span class="line">                        <span class="built_in">print</span>(<span class="string">f&quot;执行任务 <span class="subst">&#123;task_id&#125;</span>&quot;</span>)</span><br><span class="line">                        func()</span><br><span class="line">                    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">                        <span class="built_in">print</span>(<span class="string">f&quot;任务 <span class="subst">&#123;task_id&#125;</span> 执行出错: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># 更新最后执行时间</span></span><br><span class="line">                    self.tasks[task_id] = (now, delay_time, <span class="literal">True</span>, interval_time, once, func)</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># 如果是一次性任务，执行后移除</span></span><br><span class="line">                    <span class="keyword">if</span> once:</span><br><span class="line">                        <span class="keyword">del</span> self.tasks[task_id]</span><br><span class="line">                        <span class="built_in">print</span>(<span class="string">f&quot;任务 <span class="subst">&#123;task_id&#125;</span> 已完成并移除&quot;</span>)</span><br><span class="line"></span><br><span class="line">            time.sleep(<span class="number">1</span>)  <span class="comment"># 模拟 CRON_TIME_INTERVAL</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">stop</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.stop_flag = <span class="literal">True</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;定时任务服务已停止&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例任务函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">example_task</span>():</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;✅ 示例任务正在执行...&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    cron = SimpleCronJob()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1. 周期性任务：每5秒执行一次</span></span><br><span class="line">    cron.add_task(<span class="string">&quot;periodic_task&quot;</span>, <span class="keyword">lambda</span>: <span class="built_in">print</span>(<span class="string">&quot;🔁 周期性任务执行&quot;</span>), interval_seconds=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. 延迟性任务：首次延迟3秒后开始，之后每6秒执行一次</span></span><br><span class="line">    cron.add_task(<span class="string">&quot;delayed_periodic_task&quot;</span>,</span><br><span class="line">                  <span class="keyword">lambda</span>: <span class="built_in">print</span>(<span class="string">&quot;🕒 延迟+周期任务执行&quot;</span>),</span><br><span class="line">                  delay_seconds=<span class="number">3</span>,</span><br><span class="line">                  interval_seconds=<span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. 一次性任务：立即执行一次（无延迟）</span></span><br><span class="line">    cron.add_task(<span class="string">&quot;one_time_task&quot;</span>,</span><br><span class="line">                  <span class="keyword">lambda</span>: <span class="built_in">print</span>(<span class="string">&quot;📎 一次性任务执行&quot;</span>),</span><br><span class="line">                  once=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4. 一次性延迟任务：延迟5秒后执行一次</span></span><br><span class="line">    cron.add_task(<span class="string">&quot;one_time_delayed_task&quot;</span>,</span><br><span class="line">                  <span class="keyword">lambda</span>: <span class="built_in">print</span>(<span class="string">&quot;⏳ 一次性延迟任务执行&quot;</span>),</span><br><span class="line">                  delay_seconds=<span class="number">5</span>,</span><br><span class="line">                  once=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        cron.start()</span><br><span class="line">    <span class="keyword">except</span> KeyboardInterrupt:</span><br><span class="line">        cron.stop()</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Python定时任务调度&quot;&gt;&lt;a href=&quot;#Python定时任务调度&quot; class=&quot;headerlink&quot; title=&quot;Python定时任务调度&quot;&gt;&lt;/a&gt;Python定时任务调度&lt;/h1&gt;&lt;p&gt;定时任务调度（&lt;strong&gt;Scheduled Task </summary>
      
    
    
    
    <category term="python" scheme="http://example.com/categories/python/"/>
    
    <category term="项目开发" scheme="http://example.com/categories/python/%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91/"/>
    
    <category term="定时任务" scheme="http://example.com/categories/python/%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91/%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="定时" scheme="http://example.com/tags/%E5%AE%9A%E6%97%B6/"/>
    
  </entry>
  
  <entry>
    <title>python | 协程</title>
    <link href="http://example.com/2025/06/16/python%E5%8D%8F%E7%A8%8B/"/>
    <id>http://example.com/2025/06/16/python%E5%8D%8F%E7%A8%8B/</id>
    <published>2025-06-16T13:01:12.000Z</published>
    <updated>2025-06-16T13:05:52.514Z</updated>
    
    <content type="html"><![CDATA[<h1 id="python协程"><a href="#python协程" class="headerlink" title="python协程"></a>python协程</h1><ol><li><p>协程是什么？</p><p>协程是一种<strong>用户级别的线程</strong>，由程序（而不是操作系统）调度。在Python种，由async def定义，使用await暂停和恢复执行。适合处理大量IO阻塞任务（如Redis、HTTP、文件、数据库等）。</p></li><li><p>为什么要用协程?<br>目前主流语言基本上都选择了多线程作为并发设施，与线程相关的概念就是抢占式多任务（Preemptive multitasking），而与协程相关的是协作式多任务。</p><p>其实不管是进程还是线程，每次阻塞、切换都需要陷入系统调用(system call)，先让CPU跑操作系统的调度程序，然后再由调度程序决定该跑哪一个进程(线程)。<br>而且由于<strong>抢占式调度执行顺序无法确定</strong>的特点，使用线程时需要非常小心地处理同步问题，而协程完全不存在这个问题（事件驱动和异步程序也有同样的优点）。</p><p>因为协程是用户自己来编写调度逻辑的，对于我们的CPU来说，协程其实是单线程，所以CPU不用去考虑怎么调度、切换上下文，这就省去了CPU的切换开销，所以协程在一定程度上又好于多线程。</p></li><li><p>简单协程示例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">say_hello</span>():</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Hello ...&quot;</span>)</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;... World!&quot;</span>)</span><br><span class="line"></span><br><span class="line">asyncio.run(say_hello())</span><br></pre></td></tr></table></figure></li><li><p>运行Redis订阅监听、HTTP并发请求、异步文件写入、异步数据库查询的协程驱动系统代码示例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> aiohttp</span><br><span class="line"><span class="keyword">import</span> aiofiles</span><br><span class="line"><span class="keyword">import</span> aiomysql</span><br><span class="line"><span class="keyword">import</span> redis.asyncio <span class="keyword">as</span> aioredis</span><br><span class="line"></span><br><span class="line"><span class="comment"># === Redis 订阅监听 ===</span></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">redis_listener</span>():</span></span><br><span class="line">    redis = aioredis.Redis()</span><br><span class="line">    pubsub = redis.pubsub()</span><br><span class="line">    <span class="keyword">await</span> pubsub.subscribe(<span class="string">&quot;demo&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;[Redis] Subscribed to &#x27;demo&#x27;&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">for</span> msg <span class="keyword">in</span> pubsub.listen():</span><br><span class="line">        <span class="keyword">if</span> msg[<span class="string">&quot;type&quot;</span>] == <span class="string">&quot;message&quot;</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;[Redis] Received: <span class="subst">&#123;msg[<span class="string">&#x27;data&#x27;</span>].decode()&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># === 并发 HTTP 请求 ===</span></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">fetch</span>(<span class="params">session, url, i</span>):</span></span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span> session.get(url) <span class="keyword">as</span> resp:</span><br><span class="line">        text = <span class="keyword">await</span> resp.text()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;[HTTP] #<span class="subst">&#123;i&#125;</span>: <span class="subst">&#123;<span class="built_in">len</span>(text)&#125;</span> bytes from <span class="subst">&#123;url&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">http_worker</span>():</span></span><br><span class="line">    urls = [<span class="string">&quot;https://example.com&quot;</span>] * <span class="number">3</span></span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span> aiohttp.ClientSession() <span class="keyword">as</span> session:</span><br><span class="line">        tasks = [fetch(session, url, i) <span class="keyword">for</span> i, url <span class="keyword">in</span> <span class="built_in">enumerate</span>(urls)]</span><br><span class="line">        <span class="keyword">await</span> asyncio.gather(*tasks)</span><br><span class="line"></span><br><span class="line"><span class="comment"># === 异步写文件 ===</span></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">file_writer</span>():</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">        <span class="keyword">async</span> <span class="keyword">with</span> aiofiles.<span class="built_in">open</span>(<span class="string">f&quot;output_<span class="subst">&#123;i&#125;</span>.txt&quot;</span>, <span class="string">&quot;w&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">await</span> f.write(<span class="string">f&quot;[File] Written by task <span class="subst">&#123;i&#125;</span>\n&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;[File] Written file <span class="subst">&#123;i&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">await</span> asyncio.sleep(<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># === 异步 MySQL 查询 ===</span></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">mysql_worker</span>():</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        conn = <span class="keyword">await</span> aiomysql.connect(</span><br><span class="line">            host=<span class="string">&#x27;localhost&#x27;</span>, port=<span class="number">3306</span>,</span><br><span class="line">            user=<span class="string">&#x27;your_user&#x27;</span>, password=<span class="string">&#x27;your_pass&#x27;</span>,</span><br><span class="line">            db=<span class="string">&#x27;your_db&#x27;</span>, autocommit=<span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">async</span> <span class="keyword">with</span> conn.cursor() <span class="keyword">as</span> cur:</span><br><span class="line">            <span class="keyword">await</span> cur.execute(<span class="string">&quot;SELECT SLEEP(1), &#x27;Hello from DB&#x27;&quot;</span>)</span><br><span class="line">            result = <span class="keyword">await</span> cur.fetchone()</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;[MySQL] Result: <span class="subst">&#123;result&#125;</span>&quot;</span>)</span><br><span class="line">        conn.close()</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;[MySQL] Error:&quot;</span>, e)</span><br><span class="line"></span><br><span class="line"><span class="comment"># === 主函数：并发运行所有任务 ===</span></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    <span class="keyword">await</span> asyncio.gather(</span><br><span class="line">        redis_listener(),</span><br><span class="line">        http_worker(),</span><br><span class="line">        file_writer(),</span><br><span class="line">        mysql_worker(),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    asyncio.run(main())</span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;python协程&quot;&gt;&lt;a href=&quot;#python协程&quot; class=&quot;headerlink&quot; title=&quot;python协程&quot;&gt;&lt;/a&gt;python协程&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;协程是什么？&lt;/p&gt;
&lt;p&gt;协程是一种&lt;strong&gt;用户级别的线程&lt;/s</summary>
      
    
    
    
    <category term="python" scheme="http://example.com/categories/python/"/>
    
    <category term="项目开发" scheme="http://example.com/categories/python/%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91/"/>
    
    <category term="协程" scheme="http://example.com/categories/python/%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91/%E5%8D%8F%E7%A8%8B/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="协程" scheme="http://example.com/tags/%E5%8D%8F%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>python | 多线程与多进程</title>
    <link href="http://example.com/2025/06/16/python%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%A4%9A%E8%BF%9B%E7%A8%8B/"/>
    <id>http://example.com/2025/06/16/python%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%A4%9A%E8%BF%9B%E7%A8%8B/</id>
    <published>2025-06-16T13:00:59.000Z</published>
    <updated>2025-06-16T13:03:33.375Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Python多进程与多线程"><a href="#Python多进程与多线程" class="headerlink" title="Python多进程与多线程"></a>Python多进程与多线程</h1><ol><li><p>Python多进程是并行执行的吗？</p><p>答：Python多线程不是并行执行的。由于CPython解释器中有一个全局解释器锁（GIL），它会导致：</p><ul><li>同一时间内只有一个县城能执行Python字节码</li><li>对于CPU密集型任务，多线程不会并行执行，线程是轮流执行的</li></ul><p>结论：Python在CPU密集型任务（图像处理、大量计算等）中不能并行，但在I/O密集型任务（网络请求、文件读写、数据库访问等）中非常有效，可以“并发”运行多个任务。</p></li><li><p>那么在Python中如何实现真正的并行？</p><p>可使用multiprocessing多进程实现多个任务的并行，每个进程有自己独立的Python解释器和内存空间，不会收到GIL的限制。</p></li><li><p>多线程、多进程Python示例</p><p>3.1 多线程</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">worker</span>(<span class="params">n</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;[Thread] Start <span class="subst">&#123;n&#125;</span>&quot;</span>)</span><br><span class="line">    time.sleep(<span class="number">1</span>)  <span class="comment"># 模拟IO任务</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;[Thread] Done <span class="subst">&#123;n&#125;</span> -&gt; <span class="subst">&#123;n * n&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">threads = []</span><br><span class="line"></span><br><span class="line">start = time.time()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    t = threading.Thread(target=worker, args=(i,))</span><br><span class="line">    threads.append(t)</span><br><span class="line">    t.start()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> threads:</span><br><span class="line">    t.join()</span><br><span class="line">end = time.time()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Threading total time: <span class="subst">&#123;end - start:<span class="number">.2</span>f&#125;</span> seconds&quot;</span>)</span><br></pre></td></tr></table></figure><p>3.2 多进程</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Process</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">worker</span>(<span class="params">n</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;[Process <span class="subst">&#123;os.getpid()&#125;</span>] Start <span class="subst">&#123;n&#125;</span>&quot;</span>)</span><br><span class="line">    time.sleep(<span class="number">1</span>)  <span class="comment"># 模拟工作</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;[Process <span class="subst">&#123;os.getpid()&#125;</span>] Done <span class="subst">&#123;n&#125;</span> -&gt; <span class="subst">&#123;n * n&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">processes = []</span><br><span class="line"></span><br><span class="line">start = time.time()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    p = Process(target=worker, args=(i,))</span><br><span class="line">    processes.append(p)</span><br><span class="line">    p.start()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> p <span class="keyword">in</span> processes:</span><br><span class="line">    p.join()</span><br><span class="line">end = time.time()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Multiprocessing total time: <span class="subst">&#123;end - start:<span class="number">.2</span>f&#125;</span> seconds&quot;</span>)</span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Python多进程与多线程&quot;&gt;&lt;a href=&quot;#Python多进程与多线程&quot; class=&quot;headerlink&quot; title=&quot;Python多进程与多线程&quot;&gt;&lt;/a&gt;Python多进程与多线程&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Python多进程是并行执行的吗？</summary>
      
    
    
    
    <category term="python" scheme="http://example.com/categories/python/"/>
    
    <category term="项目开发" scheme="http://example.com/categories/python/%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91/"/>
    
    <category term="python多进程实现并行" scheme="http://example.com/categories/python/%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91/python%E5%A4%9A%E8%BF%9B%E7%A8%8B%E5%AE%9E%E7%8E%B0%E5%B9%B6%E8%A1%8C/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="多进程" scheme="http://example.com/tags/%E5%A4%9A%E8%BF%9B%E7%A8%8B/"/>
    
    <category term="多线程" scheme="http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>python | Redis实现发布订阅</title>
    <link href="http://example.com/2025/06/16/Redis%E5%AE%9E%E7%8E%B0%E5%8F%91%E5%B8%83%E8%AE%A2%E9%98%85/"/>
    <id>http://example.com/2025/06/16/Redis%E5%AE%9E%E7%8E%B0%E5%8F%91%E5%B8%83%E8%AE%A2%E9%98%85/</id>
    <published>2025-06-16T12:59:03.000Z</published>
    <updated>2025-06-16T13:00:07.157Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Python-Redis-发布订阅"><a href="#Python-Redis-发布订阅" class="headerlink" title="Python Redis 发布订阅"></a>Python Redis 发布订阅</h1><ol><li>Redis发布/订阅是一种广播式消息系统：<ul><li>发布者：向一个频道（channel）发送消息</li><li>订阅者：订阅一个或多个频道，等待接受消息</li><li>当有消息发布到一个频道，所有订阅该频道的客户端都会立即收到消息</li></ul></li><li>应用场景示例<ul><li>实时聊天系统：用户订阅频道，别人发言就能立刻收到</li><li>消息推送/通知中心：后台发布消息，前端订阅实时显示</li><li>分布式服务通信：多个服务通过Redis通信协调</li></ul></li><li>特点<ul><li>高性能：消息实时传递</li><li>非持久化：消息不会存储</li><li>多对多支持：一个频道支持多个订阅者，一个客户端可订阅多个频道</li><li>无确认机制：不想Kafka/RabbitMQ，由消息丢失风险</li></ul></li><li>Redis 发布订阅Python类实现（redis_pubsub.py）</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> redis</span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RedisPubSub</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, host=<span class="string">&#x27;localhost&#x27;</span>, port=<span class="number">6379</span>, db=<span class="number">0</span>, channel=<span class="string">&#x27;default&#x27;</span></span>):</span></span><br><span class="line">        self.channel = channel  <span class="comment"># 频道</span></span><br><span class="line">        self.redis = redis.Redis(host=host, port=port, db=db, decode_responses=<span class="literal">True</span>)  <span class="comment"># redis客户端</span></span><br><span class="line">        self.pubsub = self.redis.pubsub() <span class="comment"># redis发布订阅对象</span></span><br><span class="line">        self._running = <span class="literal">False</span>  <span class="comment"># 是否开启订阅</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 向self.channel频道发送消息message</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">publish</span>(<span class="params">self, message: <span class="built_in">str</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;发布消息&quot;&quot;&quot;</span></span><br><span class="line">        self.redis.publish(self.channel, message)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 订阅self.channel消息</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">subscribe</span>(<span class="params">self, callback</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        订阅并启动监听线程。</span></span><br><span class="line"><span class="string">        参数 callback: 接收一个函数，在收到消息时被调用。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">listen</span>():</span></span><br><span class="line">            <span class="comment"># 订阅self.channel频道</span></span><br><span class="line">            self.pubsub.subscribe(self.channel)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Subscribed to <span class="subst">&#123;self.channel&#125;</span>&quot;</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 监听</span></span><br><span class="line">            <span class="keyword">for</span> message <span class="keyword">in</span> self.pubsub.listen():</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> self._running:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                <span class="keyword">if</span> message[<span class="string">&#x27;type&#x27;</span>] == <span class="string">&#x27;message&#x27;</span>:</span><br><span class="line">                    callback(message[<span class="string">&#x27;data&#x27;</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 开启订阅</span></span><br><span class="line">        self._running = <span class="literal">True</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 创建监听线程</span></span><br><span class="line">        self.listen_thread = threading.Thread(target=listen, daemon=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 启动监听线程</span></span><br><span class="line">        self.listen_thread.start()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">stop</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;停止订阅&quot;&quot;&quot;</span></span><br><span class="line">        self._running = <span class="literal">False</span></span><br><span class="line">        self.pubsub.unsubscribe()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Unsubscribed from <span class="subst">&#123;self.channel&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><ol><li>发布端（publisher_demo.py）</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> redis_pubsub <span class="keyword">import</span> RedisPubSub</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">pub = RedisPubSub(channel=<span class="string">&#x27;chat&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    pub.publish(<span class="string">f&quot;Message <span class="subst">&#123;i&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Sent: Message <span class="subst">&#123;i&#125;</span>&quot;</span>)</span><br><span class="line">    time.sleep(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><ol><li>订阅端（subscriber_demo.py）</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> redis_pubsub <span class="keyword">import</span> RedisPubSub</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">handle_message</span>(<span class="params">msg</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Received: <span class="subst">&#123;msg&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">sub = RedisPubSub(channel=<span class="string">&#x27;chat&#x27;</span>)</span><br><span class="line">sub.subscribe(callback=handle_message)</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        time.sleep(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">except</span> KeyboardInterrupt:</span><br><span class="line">    sub.stop()</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Python-Redis-发布订阅&quot;&gt;&lt;a href=&quot;#Python-Redis-发布订阅&quot; class=&quot;headerlink&quot; title=&quot;Python Redis 发布订阅&quot;&gt;&lt;/a&gt;Python Redis 发布订阅&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;Redi</summary>
      
    
    
    
    <category term="python" scheme="http://example.com/categories/python/"/>
    
    <category term="项目开发" scheme="http://example.com/categories/python/%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91/"/>
    
    <category term="python Redis实现发布订阅" scheme="http://example.com/categories/python/%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91/python-Redis%E5%AE%9E%E7%8E%B0%E5%8F%91%E5%B8%83%E8%AE%A2%E9%98%85/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="发布订阅" scheme="http://example.com/tags/%E5%8F%91%E5%B8%83%E8%AE%A2%E9%98%85/"/>
    
  </entry>
  
  <entry>
    <title>python | 守护进程</title>
    <link href="http://example.com/2025/06/16/%E5%AE%88%E6%8A%A4%E8%BF%9B%E7%A8%8B/"/>
    <id>http://example.com/2025/06/16/%E5%AE%88%E6%8A%A4%E8%BF%9B%E7%A8%8B/</id>
    <published>2025-06-16T12:54:21.000Z</published>
    <updated>2025-06-16T13:09:52.383Z</updated>
    
    <content type="html"><![CDATA[<p>python第三方库/python-daemon/将当前进程变为守护进行</p><ul><li><p>守护进程</p><ul><li><p>什么是守护进程？</p><p>守护进程是一种在后台运行的进程，通常用于执行周期性或长时间运行的任务。</p></li><li><p>特点：</p><ul><li>在后台运行的进程</li><li>没有控制终端（不受键盘、tty影响）</li></ul></li></ul></li><li><p>守护进程的原始实现</p><ul><li>守护进程实现核心步骤<ul><li>fork() 一个子进程，父进程退出（脱离原始终端）</li><li>setsid() 创建新会话，脱离控制终端</li><li>第二次 fork() 防止重新获得终端</li><li>chdir(‘/‘) 切换到根目录（避免锁定工作目录）</li><li>umask(0) 重设文件权限掩码</li><li>重定向 stdin、stdout、stderr 到 /dev/null</li></ul></li></ul></li><li><p>实现守护进程的方法</p><ul><li><p>方法1</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">daemonize</span>():</span></span><br><span class="line">    <span class="comment"># 第一次 fork</span></span><br><span class="line">    pid = os.fork()</span><br><span class="line">    <span class="keyword">if</span> pid &gt; <span class="number">0</span>:</span><br><span class="line">        sys.exit(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 脱离终端，创建新会话</span></span><br><span class="line">    os.setsid()</span><br><span class="line">    os.umask(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 第二次 fork，避免重新打开终端</span></span><br><span class="line">    pid = os.fork()</span><br><span class="line">    <span class="keyword">if</span> pid &gt; <span class="number">0</span>:</span><br><span class="line">        sys.exit(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 关闭标准文件描述符并重定向到 /dev/null</span></span><br><span class="line">    sys.stdout.flush()</span><br><span class="line">    sys.stderr.flush()</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;/dev/null&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>, <span class="number">0</span>) <span class="keyword">as</span> f:</span><br><span class="line">        os.dup2(f.fileno(), sys.stdin.fileno())</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;/dev/null&#x27;</span>, <span class="string">&#x27;ab&#x27;</span>, <span class="number">0</span>) <span class="keyword">as</span> f:</span><br><span class="line">        os.dup2(f.fileno(), sys.stdout.fileno())</span><br><span class="line">        os.dup2(f.fileno(), sys.stderr.fileno())</span><br></pre></td></tr></table></figure></li><li><p>方法2（推荐使用）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> daemon</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span>():</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="comment"># 你的后台任务代码</span></span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> daemon.DaemonContext():</span><br><span class="line">    run()</span><br></pre></td></tr></table></figure></li><li><p>方法3：使用multiprocessing模块</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Process, current_process</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">task</span>():</span></span><br><span class="line">    process = current_process()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Daemon process: <span class="subst">&#123;process.daemon&#125;</span>&quot;</span>)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    process = Process(target=task, daemon=<span class="literal">True</span>)</span><br><span class="line">  process.start()</span><br><span class="line">    process.join()</span><br></pre></td></tr></table></figure></li></ul></li><li><p>示例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> daemon</span><br><span class="line"><span class="keyword">from</span> daemon <span class="keyword">import</span> pidfile</span><br><span class="line"></span><br><span class="line">LOG_PATH = <span class="string">&quot;/tmp/my_daemon.log&quot;</span></span><br><span class="line">PID_PATH = <span class="string">&quot;/tmp/my_daemon.pid&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span>():</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(LOG_PATH, <span class="string">&quot;a&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(<span class="string">f&quot;[<span class="subst">&#123;time.strftime(<span class="string">&#x27;%Y-%m-%d %H:%M:%S&#x27;</span>)&#125;</span>] Daemon is running...\n&quot;</span>)</span><br><span class="line">        time.sleep(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="keyword">with</span> daemon.DaemonContext(</span><br><span class="line">        working_directory=<span class="string">&quot;.&quot;</span>,</span><br><span class="line">        umask=<span class="number">0o002</span>,</span><br><span class="line">        pidfile=pidfile.TimeoutPIDLockFile(PID_PATH)</span><br><span class="line">    ):</span><br><span class="line">        run()</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;python第三方库/python-daemon/将当前进程变为守护进行&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;守护进程&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;什么是守护进程？&lt;/p&gt;
&lt;p&gt;守护进程是一种在后台运行的进程，通常用于执行周期性或长时间运行的任务。&lt;/p&gt;
&lt;/li&gt;
&lt;</summary>
      
    
    
    
    <category term="python" scheme="http://example.com/categories/python/"/>
    
    <category term="项目开发" scheme="http://example.com/categories/python/%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91/"/>
    
    <category term="python实现守护进程" scheme="http://example.com/categories/python/%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91/python%E5%AE%9E%E7%8E%B0%E5%AE%88%E6%8A%A4%E8%BF%9B%E7%A8%8B/"/>
    
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
    <category term="守护进程" scheme="http://example.com/tags/%E5%AE%88%E6%8A%A4%E8%BF%9B%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>Transformer | transformer代码实现</title>
    <link href="http://example.com/2025/02/13/transformer%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/"/>
    <id>http://example.com/2025/02/13/transformer%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/</id>
    <published>2025-02-13T13:47:14.000Z</published>
    <updated>2025-02-13T13:50:00.339Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p>Transformer</p><p>Transformer是一种基于自注意力机制的深度学习模型，由Google在2017年的论文“Attention is All You Need”提出。Transformer由编码器（Encoder）和解码器（Decoder）组成，结构如下图所示：</p><p><img src="/2025/02/13/transformer%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/transformer.png"></p></li><li><p>Transformer Pytorch代码实现</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, max_len=<span class="number">5000</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len, dtype=torch.<span class="built_in">float</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).<span class="built_in">float</span>() * -(math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        self.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> x + self.pe[:, :x.size(<span class="number">1</span>)].detach()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, num_heads</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MultiHeadAttention, self).__init__()</span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        <span class="keyword">assert</span> d_model % self.num_heads == <span class="number">0</span>  <span class="comment"># 确保 d_model 能被 num_heads 整除</span></span><br><span class="line"></span><br><span class="line">        self.depth = d_model // self.num_heads</span><br><span class="line"></span><br><span class="line">        self.wq = nn.Linear(d_model, d_model)</span><br><span class="line">        self.wk = nn.Linear(d_model, d_model)</span><br><span class="line">        self.wv = nn.Linear(d_model, d_model)</span><br><span class="line"></span><br><span class="line">        self.dense = nn.Linear(d_model, d_model)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">split_heads</span>(<span class="params">self, x, batch_size</span>):</span></span><br><span class="line">        x = x.view(batch_size, -<span class="number">1</span>, self.num_heads, self.depth)</span><br><span class="line">        <span class="keyword">return</span> x.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">attention</span>(<span class="params">self, query, key, value, mask=<span class="literal">None</span>, dropout=<span class="literal">None</span></span>):</span></span><br><span class="line">        matmul_qk = torch.matmul(query, key.transpose(-<span class="number">2</span>, -<span class="number">1</span>))  <span class="comment"># QK^T</span></span><br><span class="line">        dk = query.size(-<span class="number">1</span>)</span><br><span class="line">        scaled_attention_logits = matmul_qk / math.sqrt(dk)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            scaled_attention_logits += (mask * -<span class="number">1e9</span>)  <span class="comment"># 避免pad部分被注意到</span></span><br><span class="line"></span><br><span class="line">        attention_weights = torch.softmax(scaled_attention_logits, dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            attention_weights = dropout(attention_weights)</span><br><span class="line"></span><br><span class="line">        output = torch.matmul(attention_weights, value)</span><br><span class="line">        <span class="keyword">return</span> output, attention_weights</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, query, key, value, mask=<span class="literal">None</span>, dropout=<span class="literal">None</span></span>):</span></span><br><span class="line">        batch_size = query.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        query = self.split_heads(self.wq(query), batch_size)</span><br><span class="line">        key = self.split_heads(self.wk(key), batch_size)</span><br><span class="line">        value = self.split_heads(self.wv(value), batch_size)</span><br><span class="line"></span><br><span class="line">        output, attention_weights = self.attention(query, key, value, mask, dropout)</span><br><span class="line">        output = output.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>).contiguous().view(batch_size, -<span class="number">1</span>, self.d_model)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.dense(output)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FeedForwardNetwork</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, d_ff=<span class="number">2048</span>, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(FeedForwardNetwork, self).__init__()</span><br><span class="line">        self.linear1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        self.linear2 = nn.Linear(d_ff, d_model)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = F.relu(self.linear1(x))</span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        <span class="keyword">return</span> self.linear2(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, num_heads, d_ff=<span class="number">2048</span>, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(EncoderLayer, self).__init__()</span><br><span class="line">        self.attention = MultiHeadAttention(d_model, num_heads)</span><br><span class="line">        self.ffn = FeedForwardNetwork(d_model, d_ff, dropout)</span><br><span class="line">        self.layernorm1 = nn.LayerNorm(d_model)</span><br><span class="line">        self.layernorm2 = nn.LayerNorm(d_model)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, mask=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="comment"># 自注意力层</span></span><br><span class="line">        attn_output = self.attention(x, x, x, mask, self.dropout)</span><br><span class="line">        x = self.layernorm1(x + attn_output)  <span class="comment"># 残差连接 + LayerNorm</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 前馈网络层</span></span><br><span class="line">        ffn_output = self.ffn(x)</span><br><span class="line">        x = self.layernorm2(x + ffn_output)  <span class="comment"># 残差连接 + LayerNorm</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, num_heads, d_ff=<span class="number">2048</span>, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(DecoderLayer, self).__init__()</span><br><span class="line">        self.attention1 = MultiHeadAttention(d_model, num_heads)</span><br><span class="line">        self.attention2 = MultiHeadAttention(d_model, num_heads)</span><br><span class="line">        self.ffn = FeedForwardNetwork(d_model, d_ff, dropout)</span><br><span class="line">        self.layernorm1 = nn.LayerNorm(d_model)</span><br><span class="line">        self.layernorm2 = nn.LayerNorm(d_model)</span><br><span class="line">        self.layernorm3 = nn.LayerNorm(d_model)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, enc_output, look_ahead_mask=<span class="literal">None</span>, padding_mask=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="comment"># 解码器中的自注意力层</span></span><br><span class="line">        attn1_output = self.attention1(x, x, x, look_ahead_mask, self.dropout)</span><br><span class="line">        x = self.layernorm1(attn1_output + x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 编码器-解码器注意力层</span></span><br><span class="line">        attn2_output = self.attention2(x, enc_output, enc_output, padding_mask, self.dropout)</span><br><span class="line">        x = self.layernorm2(attn2_output + x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 前馈网络层</span></span><br><span class="line">        ffn_output = self.ffn(x)</span><br><span class="line">        x = self.layernorm3(ffn_output + x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerEncoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, d_model, num_heads, num_layers, d_ff=<span class="number">2048</span>, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(TransformerEncoder, self).__init__()</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, d_model)</span><br><span class="line">        self.positional_encoding = PositionalEncoding(d_model)</span><br><span class="line">        self.layers = nn.ModuleList([</span><br><span class="line">            EncoderLayer(d_model, num_heads, d_ff, dropout) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_layers)</span><br><span class="line">        ])</span><br><span class="line">        self.d_model = d_model</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, mask=<span class="literal">None</span></span>):</span></span><br><span class="line">        x = self.embedding(x) * math.sqrt(self.d_model)  <span class="comment"># 嵌入 + 缩放</span></span><br><span class="line">        x = self.positional_encoding(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, mask)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerDecoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, d_model, num_heads, num_layers, d_ff=<span class="number">2048</span>, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(TransformerDecoder, self).__init__()</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, d_model)</span><br><span class="line">        self.positional_encoding = PositionalEncoding(d_model)</span><br><span class="line">        self.layers = nn.ModuleList([</span><br><span class="line">            DecoderLayer(d_model, num_heads, d_ff, dropout) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_layers)</span><br><span class="line">        ])</span><br><span class="line">        self.d_model = d_model</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, enc_output, look_ahead_mask=<span class="literal">None</span>, padding_mask=<span class="literal">None</span></span>):</span></span><br><span class="line">        x = self.embedding(x) * math.sqrt(self.d_model)</span><br><span class="line">        x = self.positional_encoding(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, enc_output, look_ahead_mask, padding_mask)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Transformer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, d_model, num_heads, num_layers, d_ff=<span class="number">2048</span>, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Transformer, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.encoder = TransformerEncoder(vocab_size, d_model, num_heads, num_layers, d_ff, dropout)</span><br><span class="line">        self.decoder = TransformerDecoder(vocab_size, d_model, num_heads, num_layers, d_ff, dropout)</span><br><span class="line">        self.output_layer = nn.Linear(d_model, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, src, tgt, src_mask=<span class="literal">None</span>, tgt_mask=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="comment"># 编码器部分</span></span><br><span class="line">        enc_output = self.encoder(src, src_mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 解码器部分</span></span><br><span class="line">        dec_output = self.decoder(tgt, enc_output, tgt_mask, src_mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 输出层</span></span><br><span class="line">        <span class="keyword">return</span> self.output_layer(dec_output)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">vocab_size = <span class="number">10000</span>  <span class="comment"># 词汇表大小</span></span><br><span class="line">d_model = <span class="number">512</span>  <span class="comment"># 特征维度</span></span><br><span class="line">num_heads = <span class="number">8</span>  <span class="comment"># 注意力头数</span></span><br><span class="line">num_layers = <span class="number">6</span>  <span class="comment"># 编码器和解码器层数</span></span><br><span class="line">dropout = <span class="number">0.1</span>  <span class="comment"># Dropout 比例</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化 Transformer 模型</span></span><br><span class="line">transformer = Transformer(vocab_size, d_model, num_heads, num_layers, dropout=dropout)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入张量（batch_size, sequence_length）</span></span><br><span class="line">src = torch.randint(<span class="number">0</span>, vocab_size, (<span class="number">32</span>, <span class="number">100</span>))  <span class="comment"># 假设 source 语言输入 batch_size=32，序列长度=100</span></span><br><span class="line">tgt = torch.randint(<span class="number">0</span>, vocab_size, (<span class="number">32</span>, <span class="number">100</span>))  <span class="comment"># 假设 target 语言输入 batch_size=32，序列长度=100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建遮罩（假设没有 padding）</span></span><br><span class="line">src_mask = <span class="literal">None</span></span><br><span class="line">tgt_mask = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line">output = transformer(src, tgt, src_mask, tgt_mask)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Output shape:&quot;</span>, output.shape)  <span class="comment"># 输出的形状 (batch_size, tgt_sequence_length, vocab_size)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Transformer&lt;/p&gt;
&lt;p&gt;Transformer是一种基于自注意力机制的深度学习模型，由Google在2017年的论文“Attention is All You Need”提出。Transformer由编码器（Encoder）和解码器（Deco</summary>
      
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    <category term="深度学习" scheme="http://example.com/categories/AI/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Transformer" scheme="http://example.com/categories/AI/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Transformer/"/>
    
    <category term="transformer代码实现" scheme="http://example.com/categories/AI/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Transformer/transformer%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/"/>
    
    
    <category term="Transformer" scheme="http://example.com/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>Transformer | 注意力机制代码实现</title>
    <link href="http://example.com/2025/02/13/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/"/>
    <id>http://example.com/2025/02/13/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/</id>
    <published>2025-02-13T13:31:36.000Z</published>
    <updated>2025-02-13T13:46:50.698Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p>自注意力机制（Self-Attention）</p><p>自注意力机制是Transfromer中的重要组件，它通过计算Query (Q)、Key (K)、Value (V)获取token之间的相关性。Q、K、V矩阵是通过输入嵌入（或前一层的输出）与权重矩阵进行线性变换得到的。</p><p>自注意力机制的输入格式为（batch_size, seq_len, d_model）,batch_size是批次大小，seq_len是序列长度，d_model是嵌入维度。</p><p>Q、K、V的计算：Q=X x W_Q, K=X x W_K, V=X x W_V。</p><p>自注意力输出的计算：output = (QxK_T) x V。</p></li><li><p>自注意力机制pytorch代码实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SingleHeadAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, embed_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(SingleHeadAttention, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 输入的embedding维度</span></span><br><span class="line">        self.embed_size = embed_size</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义查询、键和值的线性变换</span></span><br><span class="line">        self.query_fc = nn.Linear(embed_size, embed_size)</span><br><span class="line">        self.key_fc = nn.Linear(embed_size, embed_size)</span><br><span class="line">        self.value_fc = nn.Linear(embed_size, embed_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 输出的线性变换</span></span><br><span class="line">        self.out_fc = nn.Linear(embed_size, embed_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, mask=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;X.shape: &quot;</span>, X.shape)</span><br><span class="line">        <span class="comment"># Step1: 通过线性层生成查询、键和值的向量</span></span><br><span class="line">        Q = self.query_fc(X)  <span class="comment"># (batch_size, seq_len, embed_size)</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Q.shape: &quot;</span>, Q.shape)</span><br><span class="line">        K = self.key_fc(X)  <span class="comment"># (batch_size, seq_len, embed_size)</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;K.shape: &quot;</span>, K.shape)</span><br><span class="line">        V = self.value_fc(X)  <span class="comment"># (batch_size, seq_len, embed_size)</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;V.shape: &quot;</span>, V.shape)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step2: 计算注意力得分</span></span><br><span class="line">        attention_scores = torch.matmul(Q, K.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / (self.embed_size ** <span class="number">0.5</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;attention_scores.shape: &quot;</span>, attention_scores.shape)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果有mask，应用mask</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            attention_scores = attention_scores.masked_fill(mask == <span class="number">0</span>, <span class="built_in">float</span>(<span class="string">&#x27;-inf&#x27;</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step3: 计算注意力权重(softmax)</span></span><br><span class="line">        attention_weights = F.softmax(attention_scores, dim=-<span class="number">1</span>)  <span class="comment"># (batch_size, seq_len, seq_len)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step4: 加权求和得到输出</span></span><br><span class="line">        output = torch.matmul(attention_weights, V)  <span class="comment"># (batch_size, seq_len, embed_size)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    batch_size = <span class="number">2</span></span><br><span class="line">    seq_len = <span class="number">4</span></span><br><span class="line">    embed_size = <span class="number">8</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 随机生成输入数据</span></span><br><span class="line">    X = torch.randn(batch_size, seq_len, embed_size)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建自注意力模型</span></span><br><span class="line">    attention_layer = SingleHeadAttention(embed_size)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 前向传播</span></span><br><span class="line">    output = attention_layer(X)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Output: <span class="subst">&#123;output.shape&#125;</span>&quot;</span>)  <span class="comment"># (batch_size, seq_len, embed_size)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li><p>多头注意力机制（Multi-Head Attention）</p><p>多头注意力机制是Transformer模型中的一个核心组成部分，它通过并行计算多个注意力头来捕捉输入序列的不同信息，每个注意力头都有独立的Q、K、V，能够关注输入的不同子空间，从而增强模型对不同特征的表达能力。</p><p>多头注意力计算过程：</p><ol><li>线性变换：输入的向量首先会通过不同的线性变换（权重矩阵）生成多个查询（Q）、键（K）和值（V）向量。</li><li>计算注意力：每个注意力头根据查询、键和值计算注意力权重，并通过加权求和得到一个输出。</li><li>拼接：所有头的输出会被拼接在一起。</li><li>线性变换：拼接后的结果通过一个线性变换，最终输出。</li></ol></li><li><p>多头注意力机制pytorch代码实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, embed_size, num_heads</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MultiHeadAttention, self).__init__()</span><br><span class="line">        self.embed_size = embed_size</span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        self.head_dim = embed_size // num_heads</span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> self.head_dim * num_heads == embed_size, <span class="string">&quot;Embedding size must be divisible by num_heads&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义查询、键、值的线性变换</span></span><br><span class="line">        self.query_fc = nn.Linear(embed_size, embed_size)</span><br><span class="line">        self.key_fc = nn.Linear(embed_size, embed_size)</span><br><span class="line">        self.value_fc = nn.Linear(embed_size, embed_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义输出的线性变换</span></span><br><span class="line">        self.fc_out = nn.Linear(embed_size, embed_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        batch_size = X.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 通过线性变换得到 Q, K, V</span></span><br><span class="line">        Q = self.query_fc(X)  <span class="comment"># (seq_len, batch_size, embed_size)</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Q.shape: &quot;</span>, Q.shape)</span><br><span class="line">        K = self.key_fc(X)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;K.shape: &quot;</span>, K.shape)</span><br><span class="line">        V = self.value_fc(X)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;V.shape: &quot;</span>, V.shape)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将Q, K, V 切分成多个头</span></span><br><span class="line">        Q = Q.view(X.shape[<span class="number">0</span>], batch_size, self.num_heads, self.head_dim).transpose(<span class="number">1</span>,</span><br><span class="line">                                                                                    <span class="number">2</span>)  <span class="comment"># (seq_len, batch_size, num_heads, head_dim)</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Q_multi_head.shape: &quot;</span>, Q.shape)</span><br><span class="line">        K = K.view(X.shape[<span class="number">0</span>], batch_size, self.num_heads, self.head_dim).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;K_multi_head.shape: &quot;</span>, K.shape)</span><br><span class="line">        V = V.view(X.shape[<span class="number">0</span>], batch_size, self.num_heads, self.head_dim).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;V_multi_head.shape: &quot;</span>, V.shape)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算注意力得分</span></span><br><span class="line">        energy = torch.matmul(Q, K.transpose(-<span class="number">2</span>, -<span class="number">1</span>))  <span class="comment"># (seq_len, batch_size, num_heads, seq_len)</span></span><br><span class="line">        attention = torch.softmax(energy / (self.head_dim ** <span class="number">0.5</span>), dim=-<span class="number">1</span>)  <span class="comment"># 注意力得分</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Q*K.shape&quot;</span>, attention.shape)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算加权求和的输出</span></span><br><span class="line">        out = torch.matmul(attention, V)  <span class="comment"># (seq_len, batch_size, num_heads, head_dim)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将多个头合并</span></span><br><span class="line">        out = out.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(X.shape[<span class="number">0</span>], batch_size, self.num_heads * self.head_dim)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 通过输出的线性层</span></span><br><span class="line">        out = self.fc_out(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line">embed_size = <span class="number">64</span></span><br><span class="line">num_heads = <span class="number">8</span></span><br><span class="line">seq_len = <span class="number">10</span></span><br><span class="line">batch_size = <span class="number">32</span></span><br><span class="line"></span><br><span class="line">multihead_attention = MultiHeadAttention(embed_size, num_heads)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入张量，shape: (seq_len, batch_size, embed_size)</span></span><br><span class="line">X = torch.rand(seq_len, batch_size, embed_size)</span><br><span class="line"></span><br><span class="line">out = multihead_attention(X)</span><br><span class="line"><span class="built_in">print</span>(out.shape)  <span class="comment"># (seq_len, batch_size, embed_size)</span></span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;p&gt;自注意力机制（Self-Attention）&lt;/p&gt;
&lt;p&gt;自注意力机制是Transfromer中的重要组件，它通过计算Query (Q)、Key (K)、Value (V)获取token之间的相关性。Q、K、V矩阵是通过输入嵌入（或前一层的输出）与权重矩</summary>
      
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    <category term="深度学习" scheme="http://example.com/categories/AI/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Transformer" scheme="http://example.com/categories/AI/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Transformer/"/>
    
    <category term="注意力机制代码实现" scheme="http://example.com/categories/AI/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Transformer/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/"/>
    
    
    <category term="Transformer" scheme="http://example.com/tags/Transformer/"/>
    
    <category term="注意力机制" scheme="http://example.com/tags/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"/>
    
  </entry>
  
  <entry>
    <title>Tools | AI模型可视化工具Netron</title>
    <link href="http://example.com/2025/02/10/AI%E6%A8%A1%E5%9E%8B%E5%8F%AF%E8%A7%86%E5%8C%96%E5%B7%A5%E5%85%B7Netron/"/>
    <id>http://example.com/2025/02/10/AI%E6%A8%A1%E5%9E%8B%E5%8F%AF%E8%A7%86%E5%8C%96%E5%B7%A5%E5%85%B7Netron/</id>
    <published>2025-02-10T12:08:51.000Z</published>
    <updated>2025-02-10T12:12:12.030Z</updated>
    
    <content type="html"><![CDATA[<p>Netron是一款开源的深度学习模型可视化工具，支持多种深度学习框架生成的模型（例如，PyTorch、TensorFlow、ONNX等）的可视化。</p><p>网页版工具地址：<a href="https://netron.app/">Netron</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Netron是一款开源的深度学习模型可视化工具，支持多种深度学习框架生成的模型（例如，PyTorch、TensorFlow、ONNX等）的可视化。&lt;/p&gt;
&lt;p&gt;网页版工具地址：&lt;a href=&quot;https://netron.app/&quot;&gt;Netron&lt;/a&gt;&lt;/p&gt;
</summary>
      
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    <category term="Tools" scheme="http://example.com/categories/AI/Tools/"/>
    
    <category term="Netron" scheme="http://example.com/categories/AI/Tools/Netron/"/>
    
    
    <category term="AI" scheme="http://example.com/tags/AI/"/>
    
    <category term="可视化" scheme="http://example.com/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
    <category term="Netron" scheme="http://example.com/tags/Netron/"/>
    
  </entry>
  
  <entry>
    <title>GPU | FMA</title>
    <link href="http://example.com/2024/09/13/FMA/"/>
    <id>http://example.com/2024/09/13/FMA/</id>
    <published>2024-09-13T13:23:50.000Z</published>
    <updated>2024-09-13T13:39:30.622Z</updated>
    
    <content type="html"><![CDATA[<p>什么是FMA？</p><span id="more"></span><p>GPU能够加速AI模型训练和推理速度的原因是GPU拥有众多的CUDA core、Tensor Core。</p><p>AI模型的核心计算是矩阵乘加运算，Tensor Core实现了不同浮点精度的矩阵乘加运算（FMA），从而加速了AI模型的训练、推理过程。</p><p><img src="/2024/09/13/FMA/1.png"></p><p>下面是使用FMA进行矩阵乘加运算D = A * B + C的CUDA示例代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> </span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 假设矩阵A, B, C和D都是NxN大小的方阵</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> N 1024</span></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">matrixFMA</span><span class="params">(<span class="keyword">float</span> *A, <span class="keyword">float</span> *B, <span class="keyword">float</span> *C, <span class="keyword">float</span> *D, <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> row = blockIdx.y * blockDim.y + threadIdx.y;</span><br><span class="line">    <span class="keyword">int</span> col = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (row &lt; n &amp;&amp; col &lt; n) &#123;</span><br><span class="line">        <span class="keyword">float</span> sum = <span class="number">0.0f</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> k = <span class="number">0</span>; k &lt; n; ++k) &#123;</span><br><span class="line">            <span class="comment">// 使用FMA操作计算每个元素</span></span><br><span class="line">            sum = __fmaf_rn(A[row * n + k], B[k * n + col], sum);</span><br><span class="line">        &#125;</span><br><span class="line">        D[row * n + col] = sum + C[row * n + col];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">float</span> *A, *B, *C, *D;</span><br><span class="line">    <span class="keyword">float</span> *d_A, *d_B, *d_C, *d_D;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 分配主机内存</span></span><br><span class="line">    A = <span class="keyword">new</span> <span class="keyword">float</span>[N * N];</span><br><span class="line">    B = <span class="keyword">new</span> <span class="keyword">float</span>[N * N];</span><br><span class="line">    C = <span class="keyword">new</span> <span class="keyword">float</span>[N * N];</span><br><span class="line">    D = <span class="keyword">new</span> <span class="keyword">float</span>[N * N];</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 初始化输入数据</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; N * N; i++) &#123;</span><br><span class="line">        A[i] = <span class="built_in"><span class="keyword">static_cast</span></span>(i);</span><br><span class="line">        B[i] = <span class="built_in"><span class="keyword">static_cast</span></span>(<span class="number">2</span> * i);</span><br><span class="line">        C[i] = <span class="built_in"><span class="keyword">static_cast</span></span>(<span class="number">3</span> * i);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 分配设备内存</span></span><br><span class="line">    <span class="built_in">cudaMalloc</span>(&amp;d_A, N * N * <span class="built_in"><span class="keyword">sizeof</span></span>(<span class="keyword">float</span>));</span><br><span class="line">    <span class="built_in">cudaMalloc</span>(&amp;d_B, N * N * <span class="built_in"><span class="keyword">sizeof</span></span>(<span class="keyword">float</span>));</span><br><span class="line">    <span class="built_in">cudaMalloc</span>(&amp;d_C, N * N * <span class="built_in"><span class="keyword">sizeof</span></span>(<span class="keyword">float</span>));</span><br><span class="line">    <span class="built_in">cudaMalloc</span>(&amp;d_D, N * N * <span class="built_in"><span class="keyword">sizeof</span></span>(<span class="keyword">float</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将输入数据复制到设备</span></span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(d_A, A, N * N * <span class="built_in"><span class="keyword">sizeof</span></span>(<span class="keyword">float</span>), cudaMemcpyHostToDevice);</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(d_B, B, N * N * <span class="built_in"><span class="keyword">sizeof</span></span>(<span class="keyword">float</span>), cudaMemcpyHostToDevice);</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(d_C, C, N * N * <span class="built_in"><span class="keyword">sizeof</span></span>(<span class="keyword">float</span>), cudaMemcpyHostToDevice);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 配置并启动内核</span></span><br><span class="line">    <span class="keyword">int</span> threadsPerBlock = <span class="number">16</span>;</span><br><span class="line">    <span class="keyword">int</span> blocksPerGrid = (N + threadsPerBlock - <span class="number">1</span>) / threadsPerBlock;</span><br><span class="line">    <span class="function">dim3 <span class="title">dimGrid</span><span class="params">(blocksPerGrid, blocksPerGrid, <span class="number">1</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">dimBlock</span><span class="params">(threadsPerBlock, threadsPerBlock, <span class="number">1</span>)</span></span>;</span><br><span class="line">    matrixFMA&lt;&lt;&gt;&gt;(d_A, d_B, d_C, d_D, N);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将结果从设备复制回主机</span></span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(D, d_D, N * N * <span class="built_in"><span class="keyword">sizeof</span></span>(<span class="keyword">float</span>), cudaMemcpyDeviceToHost);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 验证结果（可选）</span></span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 释放内存</span></span><br><span class="line">    <span class="built_in">cudaFree</span>(d_A);</span><br><span class="line">    <span class="built_in">cudaFree</span>(d_B);</span><br><span class="line">    <span class="built_in">cudaFree</span>(d_C);</span><br><span class="line">    <span class="built_in">cudaFree</span>(d_D);</span><br><span class="line">    <span class="keyword">delete</span>[] A;</span><br><span class="line">    <span class="keyword">delete</span>[] B;</span><br><span class="line">    <span class="keyword">delete</span>[] C;</span><br><span class="line">    <span class="keyword">delete</span>[] D;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="/2024/09/13/FMA/2.png"></p><ul><li><p>FMA与Tensor Core</p><p>Tensor Core 采用融合乘法加法（FMA）的方式来高效地处理计算任务。每个 Tensor Core 每周期能执行 <strong>4x4x4 GEMM</strong>，64 个浮点乘法累加（FMA）运算。</p></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;什么是FMA？&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    <category term="AI基础设施" scheme="http://example.com/categories/AI/AI%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/"/>
    
    <category term="GPU" scheme="http://example.com/categories/AI/AI%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/GPU/"/>
    
    <category term="FMA" scheme="http://example.com/categories/AI/AI%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/GPU/FMA/"/>
    
    
    <category term="Tensor Core" scheme="http://example.com/tags/Tensor-Core/"/>
    
    <category term="FMA" scheme="http://example.com/tags/FMA/"/>
    
  </entry>
  
  <entry>
    <title>AI | AI服务器高速互联技术</title>
    <link href="http://example.com/2024/08/29/AI%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%AB%98%E9%80%9F%E4%BA%92%E8%81%94%E6%8A%80%E6%9C%AF/"/>
    <id>http://example.com/2024/08/29/AI%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%AB%98%E9%80%9F%E4%BA%92%E8%81%94%E6%8A%80%E6%9C%AF/</id>
    <published>2024-08-29T12:41:23.000Z</published>
    <updated>2024-08-29T13:10:33.646Z</updated>
    
    <content type="html"><![CDATA[<p>三种 RDMA技术RoCE、iWARP，后两者是基于以太网的技术，IB的链路层进行了重新设计。</p><p>随着大模型以及AIGC的快速发展，AI对于算力有了更高的要求。从最开始使用单机单卡（CPU+GPU）进行DL模型训练推理，到使用单机多卡（CPU+GPUs），再到多机多卡的AI集群。</p><span id="more"></span><p>集中更多的算力可以加速AI模型的训练，但只有强大的AI算力是不够的，AI模型的训练是从大量数据中学习规律、学习知识，AI的训练过程设计大量的数据搬运，需要高带宽、低延时的数据传输。高速互联技术能够将不同的算力芯片或服务器连接在一起组成一个算力网络，并提供高速的数据传输能力。</p><p>高速互联分为结点内部计算设备的互联与结点间的互联。结点内互联又分为单节点互联与超级结点互联。单节点互联能够实现两个计算设备的互联，例如CPU与CPU之间通过UPI（Ultra Path Interconnect）进行连接，GPU与GPU可以通过PCIe（Intel于2001年开发）/<a href="https://www.nvidia.cn/design-visualization/nvlink-bridges/">NVLink（Nvidia开发）</a>/Infinity Fabric（AMD开发）进行互联，CPU与GPU之间通过PCIe或者<a href="https://zhuanlan.zhihu.com/p/676847465">CXL（Compute Express Link,Intel于2019年提出的高速互联协议）</a>进行互联。超级结点互联能够实现多个计算设备的互联，例如多GPU间的互联可以使用NVLink Switch或RDMA进行互联。</p><p>机间互联通过RDMA（Remote Direct Memory Access）进行互联，RDMA常见的有三种实现：InfiniBand、RoCE、iWarp，使用最为广泛的是IB和RoCE（RoCEv2），RoCE、iWARP，后两者是基于以太网的技术，IB的链路层进行了重新设计。<img src="/2024/08/29/AI%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%AB%98%E9%80%9F%E4%BA%92%E8%81%94%E6%8A%80%E6%9C%AF/%E9%AB%98%E9%80%9F%E4%BA%92%E8%81%94%E6%8A%80%E6%9C%AF.png"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;三种 RDMA技术RoCE、iWARP，后两者是基于以太网的技术，IB的链路层进行了重新设计。&lt;/p&gt;
&lt;p&gt;随着大模型以及AIGC的快速发展，AI对于算力有了更高的要求。从最开始使用单机单卡（CPU+GPU）进行DL模型训练推理，到使用单机多卡（CPU+GPUs），再到多机多卡的AI集群。&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    <category term="高速互联技术" scheme="http://example.com/categories/AI/%E9%AB%98%E9%80%9F%E4%BA%92%E8%81%94%E6%8A%80%E6%9C%AF/"/>
    
    <category term="AI服务器高速互联技术分类" scheme="http://example.com/categories/AI/%E9%AB%98%E9%80%9F%E4%BA%92%E8%81%94%E6%8A%80%E6%9C%AF/AI%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%AB%98%E9%80%9F%E4%BA%92%E8%81%94%E6%8A%80%E6%9C%AF%E5%88%86%E7%B1%BB/"/>
    
    
    <category term="AI" scheme="http://example.com/tags/AI/"/>
    
    <category term="高速互联技术" scheme="http://example.com/tags/%E9%AB%98%E9%80%9F%E4%BA%92%E8%81%94%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title>指令集 | CISC与RISC指令集的比较</title>
    <link href="http://example.com/2024/08/29/CISC%E4%B8%8ERISC%E6%8C%87%E4%BB%A4%E9%9B%86%E7%9A%84%E6%AF%94%E8%BE%83/"/>
    <id>http://example.com/2024/08/29/CISC%E4%B8%8ERISC%E6%8C%87%E4%BB%A4%E9%9B%86%E7%9A%84%E6%AF%94%E8%BE%83/</id>
    <published>2024-08-29T12:33:38.000Z</published>
    <updated>2024-08-29T12:40:06.272Z</updated>
    
    <content type="html"><![CDATA[<p>指令集定义了CPU可以执行的指令集合。指令集从复杂度分类可分为CISC和RISC指令集。CISC指令集最常见的是X86，Intel与AMD两大CPU巨头生产的CPU以X86架构为主。RISC指令集有Arm、RISC-V、MIPS、Alpha等，Arm指令集主要应用于移动端、嵌入式计算芯片。</p><span id="more"></span><p>以下是两种不同指令集的比较：</p><table><thead><tr><th align="center"></th><th align="center">CISC</th><th align="center">RISC</th></tr></thead><tbody><tr><td align="center">指令系统</td><td align="center">复杂，庞大</td><td align="center">简单，精简</td></tr><tr><td align="center">指令数量</td><td align="center"><code>&gt;200</code></td><td align="center"><code>&lt;100</code></td></tr><tr><td align="center">指令长度</td><td align="center">不定长</td><td align="center">定长</td></tr><tr><td align="center">可访存指令</td><td align="center">不加限制</td><td align="center">只有load/store指令</td></tr><tr><td align="center">指令执行时间</td><td align="center">相差较大</td><td align="center">大部分在一个周期内完成</td></tr><tr><td align="center">指令使用频率</td><td align="center">相差较大</td><td align="center">都比较常用</td></tr><tr><td align="center">通用寄存器数</td><td align="center">较少</td><td align="center">多</td></tr><tr><td align="center">目标代码</td><td align="center">难以利用编译优化生成高效的目标代码程序</td><td align="center">可采用编译优化生成高效执行的代码</td></tr><tr><td align="center">控制方式</td><td align="center">微程序控制</td><td align="center">组合逻辑控制</td></tr><tr><td align="center">指令流水</td><td align="center">可通过一定方式实现</td><td align="center">必须实现</td></tr></tbody></table>]]></content>
    
    
    <summary type="html">&lt;p&gt;指令集定义了CPU可以执行的指令集合。指令集从复杂度分类可分为CISC和RISC指令集。CISC指令集最常见的是X86，Intel与AMD两大CPU巨头生产的CPU以X86架构为主。RISC指令集有Arm、RISC-V、MIPS、Alpha等，Arm指令集主要应用于移动端、嵌入式计算芯片。&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    <category term="AI基础设施" scheme="http://example.com/categories/AI/AI%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/"/>
    
    <category term="CPU" scheme="http://example.com/categories/AI/AI%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/CPU/"/>
    
    <category term="指令集" scheme="http://example.com/categories/AI/AI%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/CPU/%E6%8C%87%E4%BB%A4%E9%9B%86/"/>
    
    <category term="CISC与RISC指令集的比较" scheme="http://example.com/categories/AI/AI%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/CPU/%E6%8C%87%E4%BB%A4%E9%9B%86/CISC%E4%B8%8ERISC%E6%8C%87%E4%BB%A4%E9%9B%86%E7%9A%84%E6%AF%94%E8%BE%83/"/>
    
    
    <category term="指令集" scheme="http://example.com/tags/%E6%8C%87%E4%BB%A4%E9%9B%86/"/>
    
    <category term="ISA" scheme="http://example.com/tags/ISA/"/>
    
    <category term="CISC" scheme="http://example.com/tags/CISC/"/>
    
    <category term="RISC" scheme="http://example.com/tags/RISC/"/>
    
  </entry>
  
  <entry>
    <title>AI模型轻量化 | 模型蒸馏</title>
    <link href="http://example.com/2024/08/29/%E6%A8%A1%E5%9E%8B%E8%92%B8%E9%A6%8F/"/>
    <id>http://example.com/2024/08/29/%E6%A8%A1%E5%9E%8B%E8%92%B8%E9%A6%8F/</id>
    <published>2024-08-29T12:29:59.000Z</published>
    <updated>2024-08-29T12:33:05.993Z</updated>
    
    <content type="html"><![CDATA[<p><code>模型蒸馏</code>的核心思想是在保持较高预测性能的同时，通过知识迁移的方式，将一个复杂的大模型（<code>教师模型</code>）的知识传授给一个相对简单的小模型（<code>学生模型</code>），极大地降低了模型的复杂性和计算资源需求，实现了模型的轻量化和高效化。</p><span id="more"></span><p>以下是一个简单的模型蒸馏代码示例，使用PyTorch框架实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义教师模型和学生模型</span></span><br><span class="line">teacher_model = model_xxx</span><br><span class="line">student_model = model_xxx</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数和优化器</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer_teacher = optim.optimizer1</span><br><span class="line">optimizer_student = optim.optimizer2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练数据集</span></span><br><span class="line">trainset = datasets.data_xxx</span><br><span class="line">trainloader = torch.utils.data.DataLoader()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 蒸馏过程</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(NEPOCH):</span><br><span class="line">    running_loss_teacher = <span class="number">0.0</span></span><br><span class="line">    running_loss_student = <span class="number">0.0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> trainloader:</span><br><span class="line">        <span class="comment"># 教师模型的前向传播</span></span><br><span class="line">        outputs_teacher = teacher_model(inputs)</span><br><span class="line">        loss_teacher = criterion(outputs_teacher, labels)</span><br><span class="line">        running_loss_teacher += loss_teacher.item()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 学生模型的前向传播</span></span><br><span class="line">        outputs_student = student_model(inputs)</span><br><span class="line">        loss_student = criterion(outputs_student, labels) + <span class="number">0.1</span> * torch.<span class="built_in">sum</span>((outputs_teacher - outputs_student) ** <span class="number">2</span>)</span><br><span class="line">        running_loss_student += loss_student.item()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 反向传播和参数更新</span></span><br><span class="line">        optimizer_teacher.zero_grad()</span><br><span class="line">        optimizer_student.zero_grad()</span><br><span class="line">        loss_teacher.backward()</span><br><span class="line">        optimizer_teacher.step()</span><br><span class="line">        loss_student.backward()</span><br><span class="line">        optimizer_student.step()</span><br></pre></td></tr></table></figure><p>参考链接1：<a href="https://blog.csdn.net/qq_42533357/article/details/137026170">深度学习中的模型蒸馏技术：实现流程、作用及实践案例-CSDN博客</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;code&gt;模型蒸馏&lt;/code&gt;的核心思想是在保持较高预测性能的同时，通过知识迁移的方式，将一个复杂的大模型（&lt;code&gt;教师模型&lt;/code&gt;）的知识传授给一个相对简单的小模型（&lt;code&gt;学生模型&lt;/code&gt;），极大地降低了模型的复杂性和计算资源需求，实现了模型的轻量化和高效化。&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    <category term="AI模型轻量化" scheme="http://example.com/categories/AI/AI%E6%A8%A1%E5%9E%8B%E8%BD%BB%E9%87%8F%E5%8C%96/"/>
    
    <category term="模型蒸馏" scheme="http://example.com/categories/AI/AI%E6%A8%A1%E5%9E%8B%E8%BD%BB%E9%87%8F%E5%8C%96/%E6%A8%A1%E5%9E%8B%E8%92%B8%E9%A6%8F/"/>
    
    
    <category term="模型蒸馏" scheme="http://example.com/tags/%E6%A8%A1%E5%9E%8B%E8%92%B8%E9%A6%8F/"/>
    
    <category term="轻量化" scheme="http://example.com/tags/%E8%BD%BB%E9%87%8F%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>深度学习编译器 | 深度学习编译器与推理引擎的区别</title>
    <link href="http://example.com/2024/08/23/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E5%99%A8%E4%B8%8E%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
    <id>http://example.com/2024/08/23/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E5%99%A8%E4%B8%8E%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E%E7%9A%84%E5%8C%BA%E5%88%AB/</id>
    <published>2024-08-23T14:38:54.000Z</published>
    <updated>2024-08-29T12:17:17.479Z</updated>
    
    <content type="html"><![CDATA[<p>AI编译器与推理引擎的区别。</p><span id="more"></span><p><img src="/2024/08/23/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E5%99%A8%E4%B8%8E%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E%E7%9A%84%E5%8C%BA%E5%88%AB/1.png"></p><p><img src="/2024/08/23/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E5%99%A8%E4%B8%8E%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E%E7%9A%84%E5%8C%BA%E5%88%AB/2.png"></p><p><img src="/2024/08/23/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E5%99%A8%E4%B8%8E%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E%E7%9A%84%E5%8C%BA%E5%88%AB/3.png"></p><p>从整体架构图可以看到，<strong>AI编译器就是针对具体的AI加速芯片硬件，对上层用户接触到的高级语言进行编译，为AI流程实现更加高效的执行，高级语言在AI流程表示的优化是AI编译器的重点。</strong></p><p><img src="/2024/08/23/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E5%99%A8%E4%B8%8E%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E%E7%9A%84%E5%8C%BA%E5%88%AB/4.png"></p><p>从整体架构图可以看到，<strong>图优化在推理引擎中占了很小的一部分，推理引擎聚焦于Runtime执行部分和Kernel算子内核层，为不同的硬件提供更加高效、快捷的执行Engine。</strong></p><p>参考链接1：<a href="https://zhuanlan.zhihu.com/p/669347560">AI编译器技术剖析（一）-概述 - 知乎 (zhihu.com)</a></p><p>参考链接2：<a href="https://zhuanlan.zhihu.com/p/629048218">AI编译器和推理引擎的区别 - 知乎 (zhihu.com)</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;AI编译器与推理引擎的区别。&lt;/p&gt;</summary>
    
    
    
    <category term="编译器" scheme="http://example.com/categories/%E7%BC%96%E8%AF%91%E5%99%A8/"/>
    
    <category term="深度学习编译器" scheme="http://example.com/categories/%E7%BC%96%E8%AF%91%E5%99%A8/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E5%99%A8/"/>
    
    <category term="深度学习编译器与推理引擎的区别" scheme="http://example.com/categories/%E7%BC%96%E8%AF%91%E5%99%A8/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E5%99%A8/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E5%99%A8%E4%B8%8E%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
    
    
    <category term="AI" scheme="http://example.com/tags/AI/"/>
    
    <category term="编译器" scheme="http://example.com/tags/%E7%BC%96%E8%AF%91%E5%99%A8/"/>
    
    <category term="推理引擎" scheme="http://example.com/tags/%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/"/>
    
  </entry>
  
  <entry>
    <title>VPN | SSL VPN</title>
    <link href="http://example.com/2024/08/23/SSLVPN/"/>
    <id>http://example.com/2024/08/23/SSLVPN/</id>
    <published>2024-08-23T14:33:17.000Z</published>
    <updated>2024-08-29T12:18:48.198Z</updated>
    
    <content type="html"><![CDATA[<p><code>SSL VPN</code>是以<code>SSL</code>加密技术为基础的<code>VPN</code>技术，利用<code>SSL</code>提供的安全机制，为用户<code>远程访问</code>公司内部网络提供了安全保证。</p><p>SSL VPN的典型组网架构如下：</p><p><img src="/2024/08/23/SSLVPN/1.png"></p><span id="more"></span><p>SSL VPN的三种接入方式：</p><ol><li><p>Web接入方式</p><p><img src="/2024/08/23/SSLVPN/2.png"></p></li><li><p>TCP接入方式</p><p><img src="/2024/08/23/SSLVPN/3.png"></p></li><li><p>IP接入方式</p><p><img src="/2024/08/23/SSLVPN/4.png"></p></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;code&gt;SSL VPN&lt;/code&gt;是以&lt;code&gt;SSL&lt;/code&gt;加密技术为基础的&lt;code&gt;VPN&lt;/code&gt;技术，利用&lt;code&gt;SSL&lt;/code&gt;提供的安全机制，为用户&lt;code&gt;远程访问&lt;/code&gt;公司内部网络提供了安全保证。&lt;/p&gt;
&lt;p&gt;SSL VPN的典型组网架构如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2024/08/23/SSLVPN/1.png&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="计算机基础" scheme="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"/>
    
    <category term="计算机网络" scheme="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    <category term="VPN" scheme="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/VPN/"/>
    
    <category term="SSL VPN" scheme="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/VPN/SSL-VPN/"/>
    
    
    <category term="VPN" scheme="http://example.com/tags/VPN/"/>
    
    <category term="SSL VPN" scheme="http://example.com/tags/SSL-VPN/"/>
    
  </entry>
  
  <entry>
    <title>智算网络 | Scale up和Scale out网络</title>
    <link href="http://example.com/2024/08/23/Scaleup%E5%92%8CScaleout%E7%BD%91%E7%BB%9C/"/>
    <id>http://example.com/2024/08/23/Scaleup%E5%92%8CScaleout%E7%BD%91%E7%BB%9C/</id>
    <published>2024-08-23T14:29:48.000Z</published>
    <updated>2024-08-23T14:31:41.943Z</updated>
    
    <content type="html"><![CDATA[<p>智算网络包含Scale-up网络和Scale-out网络两张网络。</p><span id="more"></span><p>Scale-up网络描述的是单个机器内GPU、CPU、内存等连接在一起构成的网络，着重单机性能的提升，例如通过增加GPU的数量或增加CPU的数量、增加内存容量来提升单机的计算效率与吞吐量。单机内部不同芯片的连接采用PCIe，GPU之间的连接也可通过NVLink进行连接。</p><p>Scale-out网络描述的是多个算力机器连接起来构成的网络，属于机间互联，目的是为了突破单机性能，通过机间互联合并算力组成一个大的算力网络为大数据处理、大模型训练提供支持。不同机器间的连接可采用RDMA（比较高效的两种是RoCEv2和IB）。</p><p>参考链接1：<a href="https://mp.weixin.qq.com/s/X9if693QD1w3rU3RNDy2Nw">智算网络中Scale-out网络和Scale-up网络的本质区别是什么？</a></p><p>参考链接2：<a href="https://mp.weixin.qq.com/s/fyPFr6aBds3dIV2sc1B6Nw">用于智算场景的Scale-up互联技术分析</a></p><p>参考链接3：<a href="https://mp.weixin.qq.com/s/b6Qf8MD-FG1Ve_PlUyFb2g">CXL，AI时代的“运力”引擎</a></p><p>参考链接4：<a href="https://mp.weixin.qq.com/s/kJZFzX0rWiPtxMkrI8i6TA">Scale-up与Scale-out有什么不同？</a></p><p>参考链接5：<a href="https://mp.weixin.qq.com/s/RyApSIT-wyrEzbiWEsvgZQ">AIGC为什么要区分Scale-out和Scale-up两张网络？</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;智算网络包含Scale-up网络和Scale-out网络两张网络。&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    <category term="AI基础设施" scheme="http://example.com/categories/AI/AI%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/"/>
    
    <category term="智算中心" scheme="http://example.com/categories/AI/AI%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/%E6%99%BA%E7%AE%97%E4%B8%AD%E5%BF%83/"/>
    
    <category term="智算网络：Scale up和Scale out网络" scheme="http://example.com/categories/AI/AI%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/%E6%99%BA%E7%AE%97%E4%B8%AD%E5%BF%83/%E6%99%BA%E7%AE%97%E7%BD%91%E7%BB%9C%EF%BC%9AScale-up%E5%92%8CScale-out%E7%BD%91%E7%BB%9C/"/>
    
    
    <category term="CPU" scheme="http://example.com/tags/CPU/"/>
    
    <category term="GPU" scheme="http://example.com/tags/GPU/"/>
    
    <category term="IB" scheme="http://example.com/tags/IB/"/>
    
    <category term="RDMA" scheme="http://example.com/tags/RDMA/"/>
    
    <category term="RoCEv2" scheme="http://example.com/tags/RoCEv2/"/>
    
    <category term="PCIe" scheme="http://example.com/tags/PCIe/"/>
    
    <category term="智算网络" scheme="http://example.com/tags/%E6%99%BA%E7%AE%97%E7%BD%91%E7%BB%9C/"/>
    
    <category term="Scale up" scheme="http://example.com/tags/Scale-up/"/>
    
    <category term="Scale out" scheme="http://example.com/tags/Scale-out/"/>
    
    <category term="AIGC" scheme="http://example.com/tags/AIGC/"/>
    
  </entry>
  
  <entry>
    <title>GPU | CUDA核心与TensorCore的区别</title>
    <link href="http://example.com/2024/08/08/CUDA%E6%A0%B8%E5%BF%83%E4%B8%8ETensorCore%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
    <id>http://example.com/2024/08/08/CUDA%E6%A0%B8%E5%BF%83%E4%B8%8ETensorCore%E7%9A%84%E5%8C%BA%E5%88%AB/</id>
    <published>2024-08-08T12:45:17.000Z</published>
    <updated>2024-08-08T12:51:05.853Z</updated>
    
    <content type="html"><![CDATA[<p><code>CUDA核心</code>和<code>Tensor Core</code>是<code>NVIDIA GPU</code>中两种不同类型的计算核心且两种核心存在明显的差别，<code>CUDA核心数量</code>和<code>Tensor Core数量</code>是反映GPU计算性能的重要参数，那么CUDA核心与Tensor Core到底是什么？</p><span id="more"></span><p><code>CUDA</code>是<code>NVIDIA</code>发明的<code>并行计算平台</code>和编程模型，<code>CUDA</code>利用<code>GPU</code>强大的<code>并行处理能力</code>提升计算的性能。<code>CUDA核心</code>主要用于执行标准的<code>浮点运算</code>（单精度或双精度），每个<code>CUDA核心</code>每个时钟周期可执行<code>乘加操作</code>，适用于各种<code>通用</code>计算任务。</p><p><code>Tensor Core</code>专为<code>深度学习</code>和<code>AI</code>工作负载设计，用于<code>加速矩阵运算</code>，特别是处理<code>半精度(FP16)</code>和<code>全精度(FP32)</code>的<code>矩阵乘法和累加操作</code>，能够优化<code>深度学习训练和推理</code>过程。第一代<code>Tensor Core</code>是随着<code>Volta</code>架构一起推出的，一代<code>Tensor Core</code>允许两个 4 x 4 FP16 矩阵相乘并添加到一个 4 x 4 FP16 或 FP32 矩阵中（如下图所示），可以实现<code>混合精度训练</code>。</p><p><img src="/2024/08/08/CUDA%E6%A0%B8%E5%BF%83%E4%B8%8ETensorCore%E7%9A%84%E5%8C%BA%E5%88%AB/MAC.png"></p><p>参考链接：<a href="https://developer.volcengine.com/articles/7387624872916353043">一文理解 GPU 张量核心（Tensor Core）</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;code&gt;CUDA核心&lt;/code&gt;和&lt;code&gt;Tensor Core&lt;/code&gt;是&lt;code&gt;NVIDIA GPU&lt;/code&gt;中两种不同类型的计算核心且两种核心存在明显的差别，&lt;code&gt;CUDA核心数量&lt;/code&gt;和&lt;code&gt;Tensor Core数量&lt;/code&gt;是反映GPU计算性能的重要参数，那么CUDA核心与Tensor Core到底是什么？&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    <category term="AI基础设施" scheme="http://example.com/categories/AI/AI%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/"/>
    
    <category term="GPU" scheme="http://example.com/categories/AI/AI%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/GPU/"/>
    
    <category term="CUDA核心与Tensor Core的区别" scheme="http://example.com/categories/AI/AI%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/GPU/CUDA%E6%A0%B8%E5%BF%83%E4%B8%8ETensor-Core%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
    
    
    <category term="GPU" scheme="http://example.com/tags/GPU/"/>
    
    <category term="CUDA" scheme="http://example.com/tags/CUDA/"/>
    
    <category term="Tensor Core" scheme="http://example.com/tags/Tensor-Core/"/>
    
  </entry>
  
  <entry>
    <title>指令集 | 指令集以及国产处理器现状</title>
    <link href="http://example.com/2024/08/08/%E6%8C%87%E4%BB%A4%E9%9B%86%E4%BB%A5%E5%8F%8A%E5%9B%BD%E4%BA%A7%E5%A4%84%E7%90%86%E5%99%A8%E7%8E%B0%E7%8A%B6/"/>
    <id>http://example.com/2024/08/08/%E6%8C%87%E4%BB%A4%E9%9B%86%E4%BB%A5%E5%8F%8A%E5%9B%BD%E4%BA%A7%E5%A4%84%E7%90%86%E5%99%A8%E7%8E%B0%E7%8A%B6/</id>
    <published>2024-08-08T12:40:13.000Z</published>
    <updated>2024-08-29T12:40:40.593Z</updated>
    
    <content type="html"><![CDATA[<ul><li>指令集以及对应的国产处理器<ul><li>CISC<ul><li>X86<ul><li>海光</li><li>兆芯</li></ul></li><li>……</li></ul></li><li>RISC<ul><li>ARM<ul><li>鲲鹏、飞腾、珠峰</li></ul></li><li>RISC-V</li><li>MIPS<ul><li>龙芯 LoongArch</li></ul></li><li>Alpha<ul><li>申威 SW_64</li></ul></li><li>……</li></ul></li></ul></li></ul><span id="more"></span><p>参考链接：<a href="https://mp.weixin.qq.com/s/XA2UtNighbmaAsi2Fua93A">一文读懂面向数据中心的高性能通用RISC-V处理器技术（上）</a></p>]]></content>
    
    
    <summary type="html">&lt;ul&gt;
&lt;li&gt;指令集以及对应的国产处理器&lt;ul&gt;
&lt;li&gt;CISC&lt;ul&gt;
&lt;li&gt;X86&lt;ul&gt;
&lt;li&gt;海光&lt;/li&gt;
&lt;li&gt;兆芯&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;……&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;RISC&lt;ul&gt;
&lt;li&gt;ARM&lt;ul&gt;
&lt;li&gt;鲲鹏、飞腾、珠峰&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;RISC-V&lt;/li&gt;
&lt;li&gt;MIPS&lt;ul&gt;
&lt;li&gt;龙芯 LoongArch&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Alpha&lt;ul&gt;
&lt;li&gt;申威 SW_64&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;……&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    <category term="AI基础设施" scheme="http://example.com/categories/AI/AI%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/"/>
    
    <category term="CPU" scheme="http://example.com/categories/AI/AI%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/CPU/"/>
    
    <category term="指令集" scheme="http://example.com/categories/AI/AI%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/CPU/%E6%8C%87%E4%BB%A4%E9%9B%86/"/>
    
    <category term="指令集以及国产处理器现状" scheme="http://example.com/categories/AI/AI%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/CPU/%E6%8C%87%E4%BB%A4%E9%9B%86/%E6%8C%87%E4%BB%A4%E9%9B%86%E4%BB%A5%E5%8F%8A%E5%9B%BD%E4%BA%A7%E5%A4%84%E7%90%86%E5%99%A8%E7%8E%B0%E7%8A%B6/"/>
    
    
    <category term="指令集" scheme="http://example.com/tags/%E6%8C%87%E4%BB%A4%E9%9B%86/"/>
    
    <category term="CISC" scheme="http://example.com/tags/CISC/"/>
    
    <category term="RISC" scheme="http://example.com/tags/RISC/"/>
    
    <category term="X86" scheme="http://example.com/tags/X86/"/>
    
    <category term="ARM" scheme="http://example.com/tags/ARM/"/>
    
    <category term="国产处理器" scheme="http://example.com/tags/%E5%9B%BD%E4%BA%A7%E5%A4%84%E7%90%86%E5%99%A8/"/>
    
    <category term="RISC-V" scheme="http://example.com/tags/RISC-V/"/>
    
  </entry>
  
  <entry>
    <title>后端优化 | 循环优化</title>
    <link href="http://example.com/2024/07/22/%E5%BE%AA%E7%8E%AF%E4%BC%98%E5%8C%96/"/>
    <id>http://example.com/2024/07/22/%E5%BE%AA%E7%8E%AF%E4%BC%98%E5%8C%96/</id>
    <published>2024-07-22T15:23:23.000Z</published>
    <updated>2024-07-22T15:30:28.594Z</updated>
    
    <content type="html"><![CDATA[<p>采用深度学习编译器对深度学习代码进行编译时，在编译器后端会对IR代码进行后端优化，循环优化就包括在后端优化中，后端优化能够加速代码的运行效率。深度学习编译器编译流程如下图所示：</p><span id="more"></span><p><img src="/2024/07/22/%E5%BE%AA%E7%8E%AF%E4%BC%98%E5%8C%96/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E5%99%A8%E7%BC%96%E8%AF%91%E6%B5%81%E7%A8%8B.png"></p><p>循环优化方式：</p><ul><li><p>循环融合（loop fusion）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sayHello</span>():</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;hello&quot;</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sayBye</span>():</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;bye&quot;</span>)</span><br><span class="line"><span class="comment"># 融合前</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000000</span>):</span><br><span class="line">    sayHello()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000000</span>):</span><br><span class="line">    sayBye()</span><br><span class="line"><span class="comment"># 融合后（将两个循环融合为一个）</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000000</span>):</span><br><span class="line">    sayHello()</span><br><span class="line">    sayBye()</span><br></pre></td></tr></table></figure></li><li><p>循环重新排序（loop reorder）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 重排序前</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000000</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"><span class="comment"># 重排序后（采用迭代次数较小的循环驱动内层迭代次数较大的循环能减少内存的消耗）</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000000</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure></li><li><p>循环展开（loop unrolling）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 展开前</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    sayHello()</span><br><span class="line"><span class="comment"># 展开后</span></span><br><span class="line">sayHello()</span><br><span class="line">sayHello()</span><br><span class="line">sayHello()</span><br><span class="line">sayHello()</span><br><span class="line">sayHello()</span><br><span class="line">sayHello()</span><br><span class="line">sayHello()</span><br><span class="line">sayHello()</span><br><span class="line">sayHello()</span><br><span class="line">sayHello()</span><br></pre></td></tr></table></figure></li><li><p>循环分块（loop tiling）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sum_2d_array</span>(<span class="params">n, A</span>):</span></span><br><span class="line">    <span class="built_in">sum</span> = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            <span class="built_in">sum</span> += A[i][j]</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sum_2d_array</span>(<span class="params">n, A</span>) &#123;</span></span><br><span class="line"><span class="function">    <span class="title">sum</span> = 0</span></span><br><span class="line"><span class="function">    <span class="title">block_size</span> = 8</span></span><br><span class="line"><span class="function">    <span class="title">for</span> <span class="title">i</span> <span class="title">in</span> <span class="title">range</span>(<span class="params"><span class="number">0</span>, n, block_size</span>):</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, n, block_size):</span><br><span class="line">    <span class="keyword">for</span> bi <span class="keyword">in</span> <span class="built_in">range</span>(i, i + block_size):</span><br><span class="line">    <span class="keyword">for</span> bj <span class="keyword">in</span> <span class="built_in">range</span>(j, j + block_size):</span><br><span class="line">    <span class="built_in">sum</span> += A[bi][bj]</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;采用深度学习编译器对深度学习代码进行编译时，在编译器后端会对IR代码进行后端优化，循环优化就包括在后端优化中，后端优化能够加速代码的运行效率。深度学习编译器编译流程如下图所示：&lt;/p&gt;</summary>
    
    
    
    <category term="编译器" scheme="http://example.com/categories/%E7%BC%96%E8%AF%91%E5%99%A8/"/>
    
    <category term="深度学习编译器" scheme="http://example.com/categories/%E7%BC%96%E8%AF%91%E5%99%A8/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E5%99%A8/"/>
    
    <category term="后端优化" scheme="http://example.com/categories/%E7%BC%96%E8%AF%91%E5%99%A8/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E5%99%A8/%E5%90%8E%E7%AB%AF%E4%BC%98%E5%8C%96/"/>
    
    <category term="循环优化" scheme="http://example.com/categories/%E7%BC%96%E8%AF%91%E5%99%A8/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E5%99%A8/%E5%90%8E%E7%AB%AF%E4%BC%98%E5%8C%96/%E5%BE%AA%E7%8E%AF%E4%BC%98%E5%8C%96/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="编译器" scheme="http://example.com/tags/%E7%BC%96%E8%AF%91%E5%99%A8/"/>
    
    <category term="后端优化" scheme="http://example.com/tags/%E5%90%8E%E7%AB%AF%E4%BC%98%E5%8C%96/"/>
    
    <category term="循环优化" scheme="http://example.com/tags/%E5%BE%AA%E7%8E%AF%E4%BC%98%E5%8C%96/"/>
    
    <category term="循环融合" scheme="http://example.com/tags/%E5%BE%AA%E7%8E%AF%E8%9E%8D%E5%90%88/"/>
    
    <category term="循环重排序" scheme="http://example.com/tags/%E5%BE%AA%E7%8E%AF%E9%87%8D%E6%8E%92%E5%BA%8F/"/>
    
    <category term="循环展开" scheme="http://example.com/tags/%E5%BE%AA%E7%8E%AF%E5%B1%95%E5%BC%80/"/>
    
    <category term="循环分块" scheme="http://example.com/tags/%E5%BE%AA%E7%8E%AF%E5%88%86%E5%9D%97/"/>
    
  </entry>
  
  <entry>
    <title>RDMA | IB与RoCEv2的对比</title>
    <link href="http://example.com/2024/07/22/IB%E4%B8%8ERDMA%E7%9A%84%E5%AF%B9%E6%AF%94/"/>
    <id>http://example.com/2024/07/22/IB%E4%B8%8ERDMA%E7%9A%84%E5%AF%B9%E6%AF%94/</id>
    <published>2024-07-22T15:22:55.000Z</published>
    <updated>2024-07-22T15:27:11.741Z</updated>
    
    <content type="html"><![CDATA[<p>Nvidia Infiniband 与 RoCEv2的对比：</p><p><img src="/2024/07/22/IB%E4%B8%8ERDMA%E7%9A%84%E5%AF%B9%E6%AF%94/IB%E4%B8%8ERDMA%E7%9A%84%E5%AF%B9%E6%AF%94.png"></p><p>参考链接：<a href="https://mp.weixin.qq.com/s/kzcrq9ycET_K7TrIQT_nSg">Infiniband和RoCEv2，以及RDMA技术的未来</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Nvidia Infiniband 与 RoCEv2的对比：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2024/07/22/IB%E4%B8%8ERDMA%E7%9A%84%E5%AF%B9%E6%AF%94/IB%E4%B8%8ERDMA%E7%9A%84%E5%AF%B9%</summary>
      
    
    
    
    <category term="计算机基础" scheme="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"/>
    
    <category term="计算机网络" scheme="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    <category term="RDMA" scheme="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/RDMA/"/>
    
    <category term="IB与RoCEv2的对比" scheme="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/RDMA/IB%E4%B8%8ERoCEv2%E7%9A%84%E5%AF%B9%E6%AF%94/"/>
    
    
    <category term="IB" scheme="http://example.com/tags/IB/"/>
    
    <category term="RDMA" scheme="http://example.com/tags/RDMA/"/>
    
    <category term="RoCEv2" scheme="http://example.com/tags/RoCEv2/"/>
    
  </entry>
  
  <entry>
    <title>TVM | TVM介绍</title>
    <link href="http://example.com/2024/07/22/TVM%E4%BB%8B%E7%BB%8D/"/>
    <id>http://example.com/2024/07/22/TVM%E4%BB%8B%E7%BB%8D/</id>
    <published>2024-07-22T15:22:16.000Z</published>
    <updated>2024-07-22T15:25:11.708Z</updated>
    
    <content type="html"><![CDATA[<p>参考文章：<a href="https://www.zhihu.com/question/532085071/answer/3154629417">(38 封私信 / 80 条消息) 深度学习编译器研发工程师的工作主要是集中于编译技术的前端、中端还是后端？ - 知乎 (zhihu.com)</a></p><p>参考视频：<a href="https://www.bilibili.com/video/BV1u6421M7jN/?spm_id_from=333.788&vd_source=0d5e0d352ee1cac3b12442c119f31bfc">【3rd-party】20240215 深度学习编译技术及TVM实践分享_哔哩哔哩_bilibili</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;参考文章：&lt;a href=&quot;https://www.zhihu.com/question/532085071/answer/3154629417&quot;&gt;(38 封私信 / 80 条消息) 深度学习编译器研发工程师的工作主要是集中于编译技术的前端、中端还是后端？ - 知乎 (zh</summary>
      
    
    
    
    <category term="编译器" scheme="http://example.com/categories/%E7%BC%96%E8%AF%91%E5%99%A8/"/>
    
    <category term="深度学习编译器" scheme="http://example.com/categories/%E7%BC%96%E8%AF%91%E5%99%A8/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E5%99%A8/"/>
    
    <category term="TVM" scheme="http://example.com/categories/%E7%BC%96%E8%AF%91%E5%99%A8/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E5%99%A8/TVM/"/>
    
    <category term="TVM介绍" scheme="http://example.com/categories/%E7%BC%96%E8%AF%91%E5%99%A8/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E5%99%A8/TVM/TVM%E4%BB%8B%E7%BB%8D/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="编译器" scheme="http://example.com/tags/%E7%BC%96%E8%AF%91%E5%99%A8/"/>
    
    <category term="TVM" scheme="http://example.com/tags/TVM/"/>
    
  </entry>
  
</feed>
